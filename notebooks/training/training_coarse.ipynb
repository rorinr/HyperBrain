{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/p/home/jusers/pierschke1/shared/HyperBrain\")\n",
    "sys.path.append(\"/p/home/jusers/pierschke1/shared/HyperBrain\\\\source\")\n",
    "sys.path.append(\"c:\\\\Users\\\\robin\\\\Documents\\\\HyperBrain\")\n",
    "sys.path.append(\"c:\\\\Users\\\\robin\\\\Documents\\\\HyperBrain\\\\source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from source.datasets.brain_dataset import BrainDataset\n",
    "from source.loftr.backbone import ResNetFPN_32_8, ResNetFPN_16_4, ResNetFPN_8_2\n",
    "from source.loftr.positional_encoding import PositionalEncoding\n",
    "from source.loftr.transformer import LocalFeatureTransformer\n",
    "from source.loftr.coarse_matching import CoarseMatching\n",
    "from source.loftr.fine_matching import FineMatching\n",
    "from source.loftr.fine_preprocess import FinePreprocess\n",
    "from source.loftr.loss import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from source.miscellaneous.evaluation import evaluate_model, read_model_evaluation_metrics\n",
    "from einops.einops import rearrange\n",
    "from source.miscellaneous.model_saving import save_model\n",
    "from source.datasets.brain_dataset import collate_fn\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crop_size = 1280\n",
    "affine_transformation_range = 0.1\n",
    "perspective_transformation_range = 0.00001\n",
    "patch_size = 32\n",
    "max_translation_shift = 0\n",
    "fine_height_width = (crop_size//patch_size)*4\n",
    "coarse_height_width = crop_size//patch_size\n",
    "images_directory = \"../../data/cyto_downscaled_3344_3904/\"\n",
    "use_train_data = True\n",
    "attention = \"linear\"\n",
    "\n",
    "dataset_train = BrainDataset(\n",
    "    images_directory=images_directory,\n",
    "    train=use_train_data,\n",
    "    affine_transformation_range=affine_transformation_range,\n",
    "    perspective_transformation_range=perspective_transformation_range,\n",
    "    crop_size=crop_size,\n",
    "    patch_size=patch_size,\n",
    "    max_translation_shift=max_translation_shift,\n",
    "    fine_height_width=fine_height_width,\n",
    "    transform=v2.Compose([v2.Normalize(mean=[0.594], std=[0.204])]),\n",
    "    load_in_gpu=True,\n",
    "    sample_both_coordinates=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0015\n",
    "use_coarse_context = False\n",
    "use_l2_with_standard_deviation = False\n",
    "temperature = (\n",
    "    0.2  # Dont decrease this value, it will yield in overflows (similarity_matrix)\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# block_dimensions_8_2 =\n",
    "# block_dimensions_16_4 = [64, 96, 128, 192]\n",
    "# block_dimensions_32_8 = [32, 64, 96, 128, 192]\n",
    "block_dimensions = [32, 64, 96, 128, 192]\n",
    "fine_feature_size = block_dimensions[2]  # 2 for 32_8, 1 for 16_4, 0 for 8_2\n",
    "coarse_feature_size = block_dimensions[-1]\n",
    "backbone = ResNetFPN_32_8(block_dimensions=block_dimensions).to(device=device)\n",
    "backbone = nn.DataParallel(backbone)\n",
    "\n",
    "positional_encoding = PositionalEncoding(coarse_feature_size).to(device=device)\n",
    "\n",
    "coarse_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=coarse_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"] * 4,\n",
    "    attention_type=attention\n",
    ").to(device=device)\n",
    "coarse_loftr = nn.DataParallel(coarse_loftr)\n",
    "\n",
    "\n",
    "coarse_matcher = CoarseMatching(temperature=temperature, confidence_threshold=0.2).to(\n",
    "    device=device\n",
    ")\n",
    "\n",
    "fine_preprocess = FinePreprocess(\n",
    "    coarse_feature_size=coarse_feature_size,\n",
    "    fine_feature_size=fine_feature_size,\n",
    "    window_size=5,\n",
    "    use_coarse_context=use_coarse_context,\n",
    ").to(device=device)\n",
    "fine_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=fine_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"],\n",
    "    attention_type=attention\n",
    ").to(device=device)\n",
    "# fine_loftr = nn.DataParallel(coarse_loftr)\n",
    "\n",
    "\n",
    "fine_matching = FineMatching(\n",
    "    return_standard_deviation=use_l2_with_standard_deviation,\n",
    ").to(device=device)\n",
    "\n",
    "params = list(backbone.parameters()) + list(coarse_loftr.parameters()) + list(fine_loftr.parameters())\n",
    "optimizer = torch.optim.Adam(params, weight_decay=weight_decay, lr=learning_rate)\n",
    "\n",
    "learning_rate_gamma = 0.85\n",
    "learning_rate_step_size = 80\n",
    "scheduler = StepLR(optimizer, step_size=learning_rate_step_size, gamma=learning_rate_gamma)\n",
    "\n",
    "coarse_loss_history = []\n",
    "fine_loss_history = []\n",
    "loss_hist_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coarse_loss = \"official_focal\"\n",
    "alpha = 0.45\n",
    "gamma = 2\n",
    "fine_loss = \"l2_std\" if use_l2_with_standard_deviation else \"l2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seen_datapoints = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_run = \"coarse_matching\"\n",
    "model_directory = \"../../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Seen Datapoints: 288, Duration: 2.18 sec, Coarse Loss: 1.9375, Fine Loss: 0.3794\n",
      "Epoch: 11, Seen Datapoints: 368, Duration: 1.97 sec, Coarse Loss: 1.8783, Fine Loss: 0.3038\n",
      "Epoch: 21, Seen Datapoints: 448, Duration: 2.04 sec, Coarse Loss: 1.4294, Fine Loss: 0.3347\n",
      "Epoch: 31, Seen Datapoints: 528, Duration: 2.02 sec, Coarse Loss: 1.2238, Fine Loss: 0.2163\n",
      "Epoch: 41, Seen Datapoints: 608, Duration: 2.12 sec, Coarse Loss: 1.2034, Fine Loss: 0.2426\n",
      "Epoch: 51, Seen Datapoints: 688, Duration: 2.11 sec, Coarse Loss: 0.8069, Fine Loss: 0.1988\n",
      "Epoch: 61, Seen Datapoints: 768, Duration: 1.98 sec, Coarse Loss: 0.7650, Fine Loss: 0.1851\n",
      "Epoch: 71, Seen Datapoints: 848, Duration: 2.01 sec, Coarse Loss: 1.6077, Fine Loss: 0.3235\n",
      "Epoch: 81, Seen Datapoints: 928, Duration: 1.98 sec, Coarse Loss: 0.7946, Fine Loss: 0.1784\n",
      "Epoch: 91, Seen Datapoints: 1008, Duration: 2.01 sec, Coarse Loss: 0.8234, Fine Loss: 0.1748\n",
      "Epoch: 101, Seen Datapoints: 1088, Duration: 2.04 sec, Coarse Loss: 0.6622, Fine Loss: 0.1625\n",
      "Epoch: 111, Seen Datapoints: 1168, Duration: 2.02 sec, Coarse Loss: 0.6287, Fine Loss: 0.1363\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for step, batch in enumerate(dataloader_train):\n",
    "        image_1_crop, image_2_crop, match_matrix, relative_coordinates, _ = batch\n",
    "        number_of_matches = match_matrix.sum()\n",
    "        # print(number_of_matches)\n",
    "\n",
    "        seen_datapoints += image_1_crop.shape[0]\n",
    "        \n",
    "        image_1_crop = image_1_crop.to(device=device)\n",
    "        image_2_crop = image_2_crop.to(device=device)\n",
    "        match_matrix = match_matrix.to(device=device)\n",
    "        relative_coordinates = relative_coordinates.to(device=device)\n",
    "\n",
    "        coarse_image_feature_1, fine_image_feature_1 = backbone(image_1_crop)\n",
    "        coarse_image_feature_2, fine_image_feature_2 = backbone(image_2_crop)\n",
    "        \n",
    "        coarse_image_feature_1 = positional_encoding(coarse_image_feature_1)\n",
    "        coarse_image_feature_2 = positional_encoding(coarse_image_feature_2)\n",
    "\n",
    "        coarse_image_feature_1 = rearrange(\n",
    "            coarse_image_feature_1, \"n c h w -> n (h w) c\"\n",
    "        )\n",
    "        coarse_image_feature_2 = rearrange(\n",
    "            coarse_image_feature_2, \"n c h w -> n (h w) c\"\n",
    "        )\n",
    "\n",
    "        coarse_image_feature_1, coarse_image_feature_2 = coarse_loftr(\n",
    "            coarse_image_feature_1, coarse_image_feature_2\n",
    "        )\n",
    "\n",
    "        coarse_matches = coarse_matcher(coarse_image_feature_1, coarse_image_feature_2)\n",
    "\n",
    "        coarse_matches_ground_truth = {\n",
    "            \"batch_indices\": match_matrix.nonzero()[:, 0],\n",
    "            \"row_indices\": match_matrix.nonzero()[:, 1],\n",
    "            \"column_indices\": match_matrix.nonzero()[:, 2],\n",
    "        }\n",
    "        \n",
    "        if number_of_matches!=0:  #TODO double check if always same number of matches in batch\n",
    "            \n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_preprocess(\n",
    "                coarse_image_feature_1=coarse_image_feature_1,\n",
    "                coarse_image_feature_2=coarse_image_feature_2,\n",
    "                fine_image_feature_1=fine_image_feature_1,\n",
    "                fine_image_feature_2=fine_image_feature_2,\n",
    "                coarse_matches=coarse_matches_ground_truth,\n",
    "                fine_height_width=fine_height_width,\n",
    "                coarse_height_width=coarse_height_width,\n",
    "            )\n",
    "\n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_loftr(\n",
    "                fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "            )\n",
    "\n",
    "            predicted_relative_coordinates = fine_matching(\n",
    "                fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "            )\n",
    "\n",
    "        if coarse_loss == \"focal\":\n",
    "            coarse_loss_value = coarse_focal_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "\n",
    "        elif coarse_loss == \"official_focal\":\n",
    "            coarse_loss_value = coarse_official_focal_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "\n",
    "        elif coarse_loss == \"cross_entropy\":\n",
    "            coarse_loss_value = coarse_cross_entropy_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "            )\n",
    "\n",
    "        if fine_loss == \"l2\" and number_of_matches != 0:\n",
    "            fine_loss_value = fine_l2_loss(\n",
    "                coordinates_predicted=predicted_relative_coordinates,\n",
    "                coordinates_ground_truth=relative_coordinates,\n",
    "            )\n",
    "\n",
    "        elif fine_loss == \"l2_std\" and number_of_matches != 0:\n",
    "            fine_loss_value = fine_l2_loss_with_standard_deviation(\n",
    "                coordinates_predicted=predicted_relative_coordinates,\n",
    "                coordinates_ground_truth=relative_coordinates,\n",
    "            )\n",
    "\n",
    "        loss = coarse_loss_value + fine_loss_value if number_of_matches != 0 else coarse_loss_value\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        coarse_loss_history.append(coarse_loss_value.cpu().item())\n",
    "        fine_loss_history.append(fine_loss_value.cpu().item())\n",
    "    \n",
    "    if epoch % 10 ==1:\n",
    "        print(f\"Epoch: {epoch}, Seen Datapoints: {seen_datapoints}, Duration: {epoch_duration:.2f} sec, Coarse Loss: {coarse_loss_value:.4f}, Fine Loss: {fine_loss_value:.4f}\")\n",
    "\n",
    "    if epoch % 20 == 1 and epoch > 1:\n",
    "        hyperparameters = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"coarse_context\": use_coarse_context,\n",
    "        \"img_size\": \"3344_3904\",\n",
    "        \"ResNet\": backbone.__class__.__name__,\n",
    "        \"affine_transformation_range\": affine_transformation_range,\n",
    "        \"perspective_transformation_range\": perspective_transformation_range,\n",
    "        \"temperature\": temperature,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"coarse_loss\": coarse_loss,\n",
    "        \"fine_loss\": fine_loss,\n",
    "        \"use_train_data\": use_train_data,\n",
    "        \"block_dimensions\": block_dimensions,\n",
    "        \"use_l2_with_standard_deviation\": use_l2_with_standard_deviation,\n",
    "        \"seen_datapoints\": seen_datapoints,\n",
    "        \"crop_size\": crop_size,\n",
    "        \"patch_size\": patch_size\n",
    "        }\n",
    "\n",
    "        if coarse_loss == \"focal\":\n",
    "            hyperparameters[\"gamma\"] = gamma\n",
    "            hyperparameters[\"alpha\"] = alpha\n",
    "\n",
    "        if scheduler:\n",
    "            hyperparameters[\"scheduler\"] = scheduler.__class__.__name__\n",
    "            hyperparameters[\"learning_rate_gamma\"] = learning_rate_gamma\n",
    "            hyperparameters[\"learning_rate_step_size\"] = learning_rate_step_size\n",
    "        \n",
    "        models = {\"backbone\": backbone, \"coarse_loftr\": coarse_loftr, \"fine_loftr\": fine_loftr}\n",
    "        model_directory = save_model(\n",
    "            models,\n",
    "            hyperparameters=hyperparameters,\n",
    "            coarse_loss_history=coarse_loss_history,\n",
    "            fine_loss_history=fine_loss_history,\n",
    "            base_path=f\"../../models/{training_run}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_hyperbrain",
   "language": "python",
   "name": "conda_hyperbrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
