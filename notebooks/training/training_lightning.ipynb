{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"c:\\\\Users\\\\robin\\\\Documents\\\\HyperBrain\")\n",
    "sys.path.append(\"c:\\\\Users\\\\robin\\\\Documents\\\\HyperBrain\\\\source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'source.lightning.setup_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrain_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BrainDataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msetup_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_lightning_loftr\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mL\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'source.lightning.setup_model'"
     ]
    }
   ],
   "source": [
    "from source.datasets.brain_dataset import BrainDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from source.lightning.setup_model import setup_lightning_loftr\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 640\n",
    "affine_transformation_range = 0.25\n",
    "perspective_transformation_range = 0.0001\n",
    "patch_size = 16\n",
    "max_translation_shift = 50\n",
    "fine_height_width = (crop_size//patch_size)*4\n",
    "coarse_height_width = crop_size//patch_size\n",
    "images_directory = \"../../data/cyto_downscaled_3344_3904/\"\n",
    "use_train_data = True\n",
    "\n",
    "dataset_train = BrainDataset(\n",
    "    images_directory=images_directory,\n",
    "    train=use_train_data,\n",
    "    affine_transformation_range=affine_transformation_range,\n",
    "    perspective_transformation_range=perspective_transformation_range,\n",
    "    crop_size=crop_size,\n",
    "    patch_size=patch_size,\n",
    "    max_translation_shift=max_translation_shift,\n",
    "    fine_height_width=fine_height_width,\n",
    "    transform=v2.Compose([v2.Normalize(mean=[0.594], std=[0.204])]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ResNetFPN_16_4'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone._get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_coarse_context = False\n",
    "clamp_predictions = True\n",
    "use_l2_with_standard_deviation = True\n",
    "temperature = 0.2\n",
    "\n",
    "# block_dimensions_8_2 = [96, 128, 192]\n",
    "# block_dimensions_16_4 = [64, 96, 128, 192]\n",
    "block_dimensions = [64, 96, 128, 192]\n",
    "fine_feature_size = block_dimensions[1]  # 1 for 16_4, 0 for 8_2\n",
    "coarse_feature_size = block_dimensions[-1]\n",
    "backbone = ResNetFPN_16_4(block_dimensions=block_dimensions)\n",
    "\n",
    "positional_encoding = PositionalEncoding(coarse_feature_size)\n",
    "\n",
    "coarse_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=coarse_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"] * 4,\n",
    ")\n",
    "\n",
    "coarse_matcher = CoarseMatching(temperature=temperature, confidence_threshold=0.2)\n",
    "\n",
    "fine_preprocess = FinePreprocess(\n",
    "    coarse_feature_size=coarse_feature_size,\n",
    "    fine_feature_size=fine_feature_size,\n",
    "    window_size=5,\n",
    "    use_coarse_context=use_coarse_context,\n",
    ")\n",
    "\n",
    "fine_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=fine_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"],\n",
    ")\n",
    "\n",
    "fine_matching = FineMatching(\n",
    "    return_standard_deviation=use_l2_with_standard_deviation,\n",
    "    clamp_predictions=clamp_predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitLoFTR(L.LightningModule):\n",
    "    def __init__(self, backbone, positional_encoding, coarse_loftr, coarse_matcher, fine_preprocess, fine_loftr, fine_matching, coarse_loss, fine_loss, alpha = None, gamma = None) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.positional_encoding = positional_encoding\n",
    "        self.coarse_loftr = coarse_loftr\n",
    "        self.coarse_matcher = coarse_matcher\n",
    "        self.fine_preprocess = fine_preprocess\n",
    "        self.fine_loftr = fine_loftr\n",
    "        self.fine_matching = fine_matching\n",
    "        self.coarse_loss = coarse_loss\n",
    "        self.fine_loss = fine_loss\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image_1_crop, image_2_crop, match_matrix, relative_coordinates, coordinate_mapping = batch\n",
    "\n",
    "        coarse_image_feature_1, fine_image_feature_1 = backbone(image_1_crop)\n",
    "        coarse_image_feature_2, fine_image_feature_2 = backbone(image_2_crop)\n",
    "        coarse_height_width = coarse_image_feature_1.shape[-1]\n",
    "        fine_height_width = fine_image_feature_1.shape[-1]\n",
    "\n",
    "        coarse_image_feature_1 = positional_encoding(coarse_image_feature_1)\n",
    "        coarse_image_feature_2 = positional_encoding(coarse_image_feature_2)\n",
    "\n",
    "        coarse_image_feature_1 = rearrange(coarse_image_feature_1, \"n c h w -> n (h w) c\")\n",
    "        coarse_image_feature_2 = rearrange(coarse_image_feature_2, \"n c h w -> n (h w) c\")\n",
    "\n",
    "        coarse_image_feature_1, coarse_image_feature_2 = coarse_loftr(coarse_image_feature_1, coarse_image_feature_2)\n",
    "\n",
    "        coarse_matches = coarse_matcher(coarse_image_feature_1, coarse_image_feature_2)\n",
    "\n",
    "        coarse_matches_ground_truth = {\n",
    "            \"batch_indices\": match_matrix.nonzero()[:, 0],\n",
    "            \"row_indices\": match_matrix.nonzero()[:, 1],\n",
    "            \"column_indices\": match_matrix.nonzero()[:, 2],\n",
    "        }\n",
    "\n",
    "        fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_preprocess(\n",
    "            coarse_image_feature_1=coarse_image_feature_1,\n",
    "            coarse_image_feature_2=coarse_image_feature_2,\n",
    "            fine_image_feature_1=fine_image_feature_1,\n",
    "            fine_image_feature_2=fine_image_feature_2,\n",
    "            coarse_matches=coarse_matches_ground_truth,\n",
    "            fine_height_width=fine_height_width,\n",
    "            coarse_height_width=coarse_height_width,\n",
    "        )\n",
    "\n",
    "        fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_loftr(\n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "        )\n",
    "\n",
    "        predicted_relative_coordinates = fine_matching(\n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "        )\n",
    "\n",
    "        if self.coarse_loss == \"focal\":\n",
    "            coarse_loss_value = coarse_focal_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "                alpha=self.alpha,\n",
    "                gamma=self.gamma,\n",
    "            )\n",
    "\n",
    "        elif self.coarse_loss == \"official_focal\":\n",
    "            coarse_loss_value = coarse_official_focal_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "                alpha=self.alpha,\n",
    "                gamma=self.gamma,\n",
    "            )\n",
    "\n",
    "        elif self.coarse_loss == \"cross_entropy\":\n",
    "            coarse_loss_value = coarse_cross_entropy_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "            )\n",
    "\n",
    "        if self.fine_loss == \"l2\":\n",
    "            fine_loss_value = fine_l2_loss(\n",
    "                coordinates_predicted=predicted_relative_coordinates,\n",
    "                coordinates_ground_truth=relative_coordinates,\n",
    "            )\n",
    "\n",
    "        elif self.fine_loss == \"l2_std\":\n",
    "            fine_loss_value = fine_l2_loss_with_standard_deviation(\n",
    "                coordinates_predicted=predicted_relative_coordinates,\n",
    "                coordinates_ground_truth=relative_coordinates,\n",
    "            )\n",
    "\n",
    "        loss = coarse_loss_value + fine_loss_value\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\robin\\miniconda3\\envs\\superbrain\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Missing logger folder: c:\\Users\\robin\\Documents\\HyperBrain\\notebooks\\training\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type                    | Params\n",
      "----------------------------------------------------------------\n",
      "0 | backbone            | ResNetFPN_16_4          | 3.2 M \n",
      "1 | positional_encoding | PositionalEncoding      | 0     \n",
      "2 | coarse_loftr        | LocalFeatureTransformer | 3.0 M \n",
      "3 | coarse_matcher      | CoarseMatching          | 0     \n",
      "4 | fine_preprocess     | FinePreprocess          | 0     \n",
      "5 | fine_loftr          | LocalFeatureTransformer | 185 K \n",
      "6 | fine_matching       | FineMatching            | 0     \n",
      "----------------------------------------------------------------\n",
      "6.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 M     Total params\n",
      "25.193    Total estimated model params size (MB)\n",
      "c:\\Users\\robin\\miniconda3\\envs\\superbrain\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\robin\\miniconda3\\envs\\superbrain\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 7/7 [00:17<00:00,  0.40it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 7/7 [00:17<00:00,  0.39it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "loftr = LitLoFTR(backbone, positional_encoding, coarse_loftr, coarse_matcher, fine_preprocess, fine_loftr, fine_matching, \"focal\", \"l2_std\", 0.7, 2.5)\n",
    "trainer = L.Trainer(max_epochs=1)\n",
    "trainer.fit(loftr, DataLoader(dataset_train, batch_size=1, shuffle=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
