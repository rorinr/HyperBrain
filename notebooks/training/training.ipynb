{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/p/home/jusers/pierschke1/shared/HyperBrain\")\n",
    "sys.path.append(\"/p/home/jusers/pierschke1/shared/HyperBrain\\\\source\")\n",
    "sys.path.append(\"c:\\\\Users\\\\robin\\\\Documents\\\\HyperBrain\")\n",
    "sys.path.append(\"c:\\\\Users\\\\robin\\\\Documents\\\\HyperBrain\\\\source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from source.datasets.brain_dataset import BrainDataset\n",
    "from source.loftr.backbone import ResNetFPN_16_4, ResNetFPN_8_2\n",
    "from source.loftr.positional_encoding import PositionalEncoding\n",
    "from source.loftr.transformer import LocalFeatureTransformer\n",
    "from source.loftr.coarse_matching import CoarseMatching\n",
    "from source.loftr.fine_matching import FineMatching\n",
    "from source.loftr.fine_preprocess import FinePreprocess\n",
    "from source.loftr.loss import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from source.miscellaneous.evaluation import evaluate_model, read_model_evaluation_metrics\n",
    "from einops.einops import rearrange\n",
    "from source.miscellaneous.model_saving import save_model\n",
    "from source.datasets.brain_dataset import collate_fn\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crop_size = 480\n",
    "affine_transformation_range = 0.25\n",
    "perspective_transformation_range = 0.000125\n",
    "patch_size = 8\n",
    "max_translation_shift = 300\n",
    "fine_height_width = (crop_size//patch_size)*4\n",
    "coarse_height_width = crop_size//patch_size\n",
    "images_directory = \"../../data/cyto_downscaled_3344_3904\"\n",
    "use_train_data = True\n",
    "attention = \"linear\"\n",
    "\n",
    "dataset_train = BrainDataset(\n",
    "    images_directory=images_directory,\n",
    "    train=use_train_data,\n",
    "    affine_transformation_range=affine_transformation_range,\n",
    "    perspective_transformation_range=perspective_transformation_range,\n",
    "    crop_size=crop_size,\n",
    "    patch_size=patch_size,\n",
    "    max_translation_shift=max_translation_shift,\n",
    "    fine_height_width=fine_height_width,\n",
    "    transform=v2.Compose([v2.Normalize(mean=[0.594], std=[0.204])]),\n",
    "    load_in_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0015\n",
    "use_coarse_context = False\n",
    "use_l2_with_standard_deviation = False\n",
    "temperature = (\n",
    "    0.2  # Dont decrease this value, it will yield in overflows (similarity_matrix)\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# block_dimensions_8_2 = [96, 128, 192]\n",
    "# block_dimensions_16_4 = [64, 96, 128, 192]\n",
    "block_dimensions = [128, 196, 256]\n",
    "fine_feature_size = block_dimensions[0]  # 1 for 16_4, 0 for 8_2\n",
    "coarse_feature_size = block_dimensions[-1]\n",
    "backbone = ResNetFPN_8_2(block_dimensions=block_dimensions).to(device=device)\n",
    "backbone = nn.DataParallel(backbone)\n",
    "\n",
    "positional_encoding = PositionalEncoding(coarse_feature_size).to(device=device)\n",
    "\n",
    "coarse_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=coarse_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"] * 4,\n",
    "    attention_type=attention\n",
    ").to(device=device)\n",
    "coarse_loftr = nn.DataParallel(coarse_loftr)\n",
    "\n",
    "\n",
    "coarse_matcher = CoarseMatching(temperature=temperature, confidence_threshold=0.2).to(\n",
    "    device=device\n",
    ")\n",
    "\n",
    "fine_preprocess = FinePreprocess(\n",
    "    coarse_feature_size=coarse_feature_size,\n",
    "    fine_feature_size=fine_feature_size,\n",
    "    window_size=5,\n",
    "    use_coarse_context=use_coarse_context,\n",
    ").to(device=device)\n",
    "fine_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=fine_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"],\n",
    "    attention_type=attention\n",
    ").to(device=device)\n",
    "# fine_loftr = nn.DataParallel(coarse_loftr)\n",
    "\n",
    "\n",
    "fine_matching = FineMatching(\n",
    "    return_standard_deviation=use_l2_with_standard_deviation,\n",
    ").to(device=device)\n",
    "\n",
    "params = list(backbone.parameters()) + list(coarse_loftr.parameters()) + list(fine_loftr.parameters())\n",
    "optimizer = torch.optim.Adam(params, weight_decay=weight_decay, lr=learning_rate)\n",
    "\n",
    "learning_rate_gamma = 0.9\n",
    "learning_rate_step_size = 100\n",
    "scheduler = StepLR(optimizer, step_size=learning_rate_step_size, gamma=learning_rate_gamma)\n",
    "\n",
    "coarse_loss_history = []\n",
    "fine_loss_history = []\n",
    "loss_hist_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coarse_loss = \"focal\"\n",
    "alpha = 0.45\n",
    "gamma = 2\n",
    "fine_loss = \"l2_std\" if use_l2_with_standard_deviation else \"l2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seen_datapoints = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_run = \"run_x0025\"\n",
    "model_directory = \"../../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Seen Datapoints: 80, Duration: 1.20 sec, Coarse Loss: 6.2170, Fine Loss: 0.6415\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 20, Seen Datapoints: 160, Duration: 1.30 sec, Coarse Loss: 6.2170, Fine Loss: 0.6133\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 30, Seen Datapoints: 240, Duration: 1.23 sec, Coarse Loss: 6.1956, Fine Loss: 0.6247\n",
      "Epoch: 40, Seen Datapoints: 320, Duration: 1.23 sec, Coarse Loss: 6.1918, Fine Loss: 0.5940\n",
      "Epoch: 50, Seen Datapoints: 400, Duration: 1.26 sec, Coarse Loss: 6.0813, Fine Loss: 0.5953\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 60, Seen Datapoints: 480, Duration: 1.37 sec, Coarse Loss: 5.8827, Fine Loss: 0.6138\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 70, Seen Datapoints: 560, Duration: 1.24 sec, Coarse Loss: 5.7026, Fine Loss: 0.5525\n",
      "Epoch: 80, Seen Datapoints: 640, Duration: 1.23 sec, Coarse Loss: 5.8333, Fine Loss: 0.5933\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 90, Seen Datapoints: 720, Duration: 1.31 sec, Coarse Loss: 5.5900, Fine Loss: 0.6507\n",
      "Epoch: 100, Seen Datapoints: 800, Duration: 1.27 sec, Coarse Loss: 4.9891, Fine Loss: 0.5582\n",
      "Epoch: 110, Seen Datapoints: 880, Duration: 1.27 sec, Coarse Loss: 4.9390, Fine Loss: 0.5749\n",
      "Epoch: 120, Seen Datapoints: 960, Duration: 1.27 sec, Coarse Loss: 4.8338, Fine Loss: 0.5779\n",
      "Epoch: 130, Seen Datapoints: 1040, Duration: 1.28 sec, Coarse Loss: 4.2268, Fine Loss: 0.5733\n",
      "Epoch: 140, Seen Datapoints: 1120, Duration: 1.21 sec, Coarse Loss: 4.6975, Fine Loss: 0.5228\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 150, Seen Datapoints: 1200, Duration: 1.22 sec, Coarse Loss: 5.0782, Fine Loss: 0.6176\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 160, Seen Datapoints: 1280, Duration: 1.25 sec, Coarse Loss: 4.0713, Fine Loss: 0.4878\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 170, Seen Datapoints: 1360, Duration: 1.30 sec, Coarse Loss: 4.2023, Fine Loss: 0.5574\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 180, Seen Datapoints: 1440, Duration: 1.27 sec, Coarse Loss: 3.8178, Fine Loss: 0.5235\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 190, Seen Datapoints: 1520, Duration: 1.22 sec, Coarse Loss: 3.9161, Fine Loss: 0.5214\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 200, Seen Datapoints: 1600, Duration: 1.25 sec, Coarse Loss: 4.0659, Fine Loss: 0.5158\n",
      "Epoch: 210, Seen Datapoints: 1680, Duration: 1.22 sec, Coarse Loss: 3.7055, Fine Loss: 0.5234\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 220, Seen Datapoints: 1760, Duration: 1.25 sec, Coarse Loss: 4.0117, Fine Loss: 0.4943\n",
      "Epoch: 230, Seen Datapoints: 1840, Duration: 1.28 sec, Coarse Loss: 3.5379, Fine Loss: 0.5026\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 240, Seen Datapoints: 1920, Duration: 1.22 sec, Coarse Loss: 3.8507, Fine Loss: 0.5378\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 250, Seen Datapoints: 2000, Duration: 1.22 sec, Coarse Loss: 3.8586, Fine Loss: 0.4831\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 260, Seen Datapoints: 2080, Duration: 1.21 sec, Coarse Loss: 3.8148, Fine Loss: 0.5573\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 270, Seen Datapoints: 2160, Duration: 1.21 sec, Coarse Loss: 3.9611, Fine Loss: 0.5940\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 280, Seen Datapoints: 2240, Duration: 1.20 sec, Coarse Loss: 3.1028, Fine Loss: 0.5600\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 290, Seen Datapoints: 2320, Duration: 1.32 sec, Coarse Loss: 3.4048, Fine Loss: 0.5289\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 300, Seen Datapoints: 2400, Duration: 1.17 sec, Coarse Loss: 3.7654, Fine Loss: 0.5510\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 310, Seen Datapoints: 2480, Duration: 1.24 sec, Coarse Loss: 3.8240, Fine Loss: 0.4923\n",
      "Epoch: 320, Seen Datapoints: 2560, Duration: 1.29 sec, Coarse Loss: 3.2180, Fine Loss: 0.4756\n",
      "Epoch: 330, Seen Datapoints: 2640, Duration: 1.25 sec, Coarse Loss: 3.2664, Fine Loss: 0.5172\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 340, Seen Datapoints: 2720, Duration: 1.23 sec, Coarse Loss: 3.0090, Fine Loss: 0.5151\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 350, Seen Datapoints: 2800, Duration: 1.33 sec, Coarse Loss: 3.7645, Fine Loss: 0.5484\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 360, Seen Datapoints: 2880, Duration: 1.24 sec, Coarse Loss: 3.3853, Fine Loss: 0.4555\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 370, Seen Datapoints: 2960, Duration: 1.19 sec, Coarse Loss: 5.2569, Fine Loss: 0.6474\n",
      "Epoch: 380, Seen Datapoints: 3040, Duration: 1.23 sec, Coarse Loss: 3.4657, Fine Loss: 0.5125\n",
      "Epoch: 390, Seen Datapoints: 3120, Duration: 1.21 sec, Coarse Loss: 4.1868, Fine Loss: 0.5566\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 400, Seen Datapoints: 3200, Duration: 1.27 sec, Coarse Loss: 3.2283, Fine Loss: 0.5013\n",
      "Epoch: 410, Seen Datapoints: 3280, Duration: 1.18 sec, Coarse Loss: 3.6349, Fine Loss: 0.5852\n",
      "Epoch: 420, Seen Datapoints: 3360, Duration: 1.29 sec, Coarse Loss: 2.9119, Fine Loss: 0.5282\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 430, Seen Datapoints: 3440, Duration: 1.24 sec, Coarse Loss: 3.1918, Fine Loss: 0.5377\n",
      "Epoch: 440, Seen Datapoints: 3520, Duration: 1.23 sec, Coarse Loss: 3.6558, Fine Loss: 0.5332\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 450, Seen Datapoints: 3600, Duration: 1.26 sec, Coarse Loss: 3.6621, Fine Loss: 0.5373\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 460, Seen Datapoints: 3680, Duration: 1.31 sec, Coarse Loss: 3.4594, Fine Loss: 0.4909\n",
      "Epoch: 470, Seen Datapoints: 3760, Duration: 1.36 sec, Coarse Loss: 2.8809, Fine Loss: 0.4761\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 480, Seen Datapoints: 3840, Duration: 1.24 sec, Coarse Loss: 3.1499, Fine Loss: 0.5334\n",
      "Epoch: 490, Seen Datapoints: 3920, Duration: 1.27 sec, Coarse Loss: 3.7949, Fine Loss: 0.5612\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 500, Seen Datapoints: 4000, Duration: 1.23 sec, Coarse Loss: 3.0492, Fine Loss: 0.4621\n",
      "Epoch: 510, Seen Datapoints: 4080, Duration: 1.45 sec, Coarse Loss: 2.7712, Fine Loss: 0.4348\n",
      "Epoch: 520, Seen Datapoints: 4160, Duration: 1.26 sec, Coarse Loss: 2.8260, Fine Loss: 0.4759\n",
      "Epoch: 530, Seen Datapoints: 4240, Duration: 1.33 sec, Coarse Loss: 3.2962, Fine Loss: 0.5166\n",
      "Epoch: 540, Seen Datapoints: 4320, Duration: 1.25 sec, Coarse Loss: 2.9951, Fine Loss: 0.4982\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 550, Seen Datapoints: 4400, Duration: 1.28 sec, Coarse Loss: 4.7621, Fine Loss: 0.6477\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 560, Seen Datapoints: 4480, Duration: 1.44 sec, Coarse Loss: 2.6278, Fine Loss: 0.4710\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 570, Seen Datapoints: 4560, Duration: 1.31 sec, Coarse Loss: 3.3986, Fine Loss: 0.4740\n",
      "Epoch: 580, Seen Datapoints: 4640, Duration: 1.35 sec, Coarse Loss: 2.7911, Fine Loss: 0.5283\n",
      "Epoch: 590, Seen Datapoints: 4720, Duration: 1.23 sec, Coarse Loss: 3.5667, Fine Loss: 0.5956\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 600, Seen Datapoints: 4800, Duration: 1.30 sec, Coarse Loss: 2.3068, Fine Loss: 0.4707\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 610, Seen Datapoints: 4880, Duration: 1.30 sec, Coarse Loss: 3.5141, Fine Loss: 0.5400\n",
      "Epoch: 620, Seen Datapoints: 4960, Duration: 1.26 sec, Coarse Loss: 2.1392, Fine Loss: 0.4581\n",
      "Epoch: 630, Seen Datapoints: 5040, Duration: 1.18 sec, Coarse Loss: 2.9351, Fine Loss: 0.5367\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 640, Seen Datapoints: 5120, Duration: 1.20 sec, Coarse Loss: 2.8632, Fine Loss: 0.5099\n",
      "Epoch: 650, Seen Datapoints: 5200, Duration: 1.28 sec, Coarse Loss: 2.4766, Fine Loss: 0.4628\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 660, Seen Datapoints: 5280, Duration: 1.28 sec, Coarse Loss: 3.2171, Fine Loss: 0.5400\n",
      "Epoch: 670, Seen Datapoints: 5360, Duration: 1.25 sec, Coarse Loss: 2.7303, Fine Loss: 0.4979\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 680, Seen Datapoints: 5440, Duration: 1.20 sec, Coarse Loss: 3.1553, Fine Loss: 0.5253\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 690, Seen Datapoints: 5520, Duration: 1.27 sec, Coarse Loss: 3.0010, Fine Loss: 0.4939\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 700, Seen Datapoints: 5600, Duration: 1.33 sec, Coarse Loss: 3.0339, Fine Loss: 0.4966\n",
      "Epoch: 710, Seen Datapoints: 5680, Duration: 1.26 sec, Coarse Loss: 2.7210, Fine Loss: 0.4925\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 720, Seen Datapoints: 5760, Duration: 1.29 sec, Coarse Loss: 2.7985, Fine Loss: 0.4870\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 730, Seen Datapoints: 5840, Duration: 1.24 sec, Coarse Loss: 2.9553, Fine Loss: 0.4905\n",
      "Epoch: 740, Seen Datapoints: 5920, Duration: 1.30 sec, Coarse Loss: 2.7682, Fine Loss: 0.4868\n",
      "Epoch: 750, Seen Datapoints: 6000, Duration: 1.25 sec, Coarse Loss: 3.0573, Fine Loss: 0.4731\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 760, Seen Datapoints: 6080, Duration: 1.17 sec, Coarse Loss: 2.9060, Fine Loss: 0.5330\n",
      "Epoch: 770, Seen Datapoints: 6160, Duration: 1.24 sec, Coarse Loss: 2.7273, Fine Loss: 0.5054\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 780, Seen Datapoints: 6240, Duration: 1.20 sec, Coarse Loss: 2.8042, Fine Loss: 0.5359\n",
      "Epoch: 790, Seen Datapoints: 6320, Duration: 1.23 sec, Coarse Loss: 2.6484, Fine Loss: 0.4705\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 800, Seen Datapoints: 6400, Duration: 1.26 sec, Coarse Loss: 2.6445, Fine Loss: 0.4893\n",
      "Epoch: 810, Seen Datapoints: 6480, Duration: 1.20 sec, Coarse Loss: 2.7061, Fine Loss: 0.4921\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 820, Seen Datapoints: 6560, Duration: 1.32 sec, Coarse Loss: 2.2855, Fine Loss: 0.4321\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 830, Seen Datapoints: 6640, Duration: 1.20 sec, Coarse Loss: 2.8000, Fine Loss: 0.4639\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 840, Seen Datapoints: 6720, Duration: 1.30 sec, Coarse Loss: 2.5088, Fine Loss: 0.4645\n",
      "Epoch: 850, Seen Datapoints: 6800, Duration: 1.25 sec, Coarse Loss: 2.8135, Fine Loss: 0.4946\n",
      "Epoch: 860, Seen Datapoints: 6880, Duration: 1.28 sec, Coarse Loss: 2.2402, Fine Loss: 0.4477\n",
      "Epoch: 870, Seen Datapoints: 6960, Duration: 1.19 sec, Coarse Loss: 2.7127, Fine Loss: 0.4766\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 880, Seen Datapoints: 7040, Duration: 1.22 sec, Coarse Loss: 3.1827, Fine Loss: 0.5109\n",
      "Epoch: 890, Seen Datapoints: 7120, Duration: 1.27 sec, Coarse Loss: 2.0982, Fine Loss: 0.4069\n",
      "Epoch: 900, Seen Datapoints: 7200, Duration: 1.31 sec, Coarse Loss: 2.3488, Fine Loss: 0.4671\n",
      "Epoch: 910, Seen Datapoints: 7280, Duration: 1.21 sec, Coarse Loss: 2.4787, Fine Loss: 0.4507\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 920, Seen Datapoints: 7360, Duration: 1.31 sec, Coarse Loss: 2.7692, Fine Loss: 0.4837\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 930, Seen Datapoints: 7440, Duration: 1.25 sec, Coarse Loss: 2.1454, Fine Loss: 0.4279\n",
      "Epoch: 940, Seen Datapoints: 7520, Duration: 1.28 sec, Coarse Loss: 2.6197, Fine Loss: 0.5113\n",
      "Epoch: 950, Seen Datapoints: 7600, Duration: 1.34 sec, Coarse Loss: 1.9630, Fine Loss: 0.3750\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 960, Seen Datapoints: 7680, Duration: 1.17 sec, Coarse Loss: 1.8551, Fine Loss: 0.4337\n",
      "Epoch: 970, Seen Datapoints: 7760, Duration: 1.27 sec, Coarse Loss: 2.4587, Fine Loss: 0.4288\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 980, Seen Datapoints: 7840, Duration: 1.32 sec, Coarse Loss: 2.2284, Fine Loss: 0.4434\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 990, Seen Datapoints: 7920, Duration: 1.21 sec, Coarse Loss: 2.5385, Fine Loss: 0.5042\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 1000, Seen Datapoints: 8000, Duration: 1.24 sec, Coarse Loss: 2.4921, Fine Loss: 0.5495\n",
      "Epoch: 1010, Seen Datapoints: 8080, Duration: 1.33 sec, Coarse Loss: 2.7671, Fine Loss: 0.5426\n",
      "Epoch: 1020, Seen Datapoints: 8160, Duration: 1.21 sec, Coarse Loss: 2.3530, Fine Loss: 0.4345\n",
      "Epoch: 1030, Seen Datapoints: 8240, Duration: 1.24 sec, Coarse Loss: 2.3100, Fine Loss: 0.4118\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 1040, Seen Datapoints: 8320, Duration: 1.25 sec, Coarse Loss: 2.6341, Fine Loss: 0.5449\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 1050, Seen Datapoints: 8400, Duration: 1.24 sec, Coarse Loss: 3.0928, Fine Loss: 0.4984\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 1060, Seen Datapoints: 8480, Duration: 1.27 sec, Coarse Loss: 2.6682, Fine Loss: 0.4662\n",
      "Epoch: 1070, Seen Datapoints: 8560, Duration: 1.20 sec, Coarse Loss: 2.5362, Fine Loss: 0.4626\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 1080, Seen Datapoints: 8640, Duration: 1.23 sec, Coarse Loss: 2.5990, Fine Loss: 0.4090\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 1090, Seen Datapoints: 8720, Duration: 1.22 sec, Coarse Loss: 3.9066, Fine Loss: 0.5434\n",
      "Epoch: 1100, Seen Datapoints: 8800, Duration: 1.23 sec, Coarse Loss: 1.9744, Fine Loss: 0.4522\n",
      "Epoch: 1110, Seen Datapoints: 8880, Duration: 1.22 sec, Coarse Loss: 2.0679, Fine Loss: 0.4502\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 1120, Seen Datapoints: 8960, Duration: 1.28 sec, Coarse Loss: 3.5037, Fine Loss: 0.5277\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 1130, Seen Datapoints: 9040, Duration: 1.32 sec, Coarse Loss: 3.5130, Fine Loss: 0.5431\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m match_matrix \u001b[38;5;241m=\u001b[39m match_matrix\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     14\u001b[0m relative_coordinates \u001b[38;5;241m=\u001b[39m relative_coordinates\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 16\u001b[0m coarse_image_feature_1, fine_image_feature_1 \u001b[38;5;241m=\u001b[39m backbone(image_1_crop)\n\u001b[1;32m     17\u001b[0m coarse_image_feature_2, fine_image_feature_2 \u001b[38;5;241m=\u001b[39m backbone(image_2_crop)\n\u001b[1;32m     19\u001b[0m coarse_image_feature_1 \u001b[38;5;241m=\u001b[39m positional_encoding(coarse_image_feature_1)\n",
      "File \u001b[0;32m~/PROJECT_training2005/testdir/miniconda3/envs/hyperbrain/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/PROJECT_training2005/testdir/miniconda3/envs/hyperbrain/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PROJECT_training2005/testdir/miniconda3/envs/hyperbrain/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/PROJECT_training2005/testdir/miniconda3/envs/hyperbrain/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(replicas)])\n",
      "File \u001b[0;32m~/PROJECT_training2005/testdir/miniconda3/envs/hyperbrain/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:100\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     98\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m--> 100\u001b[0m         thread\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/PROJECT_training2005/testdir/miniconda3/envs/hyperbrain/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/PROJECT_training2005/testdir/miniconda3/envs/hyperbrain/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1500\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for step, batch in enumerate(dataloader_train):\n",
    "        image_1_crop, image_2_crop, match_matrix, relative_coordinates, _ = batch\n",
    "\n",
    "        seen_datapoints += image_1_crop.shape[0]\n",
    "        \n",
    "        image_1_crop = image_1_crop.to(device=device)\n",
    "        image_2_crop = image_2_crop.to(device=device)\n",
    "        match_matrix = match_matrix.to(device=device)\n",
    "        relative_coordinates = relative_coordinates.to(device=device)\n",
    "\n",
    "        coarse_image_feature_1, fine_image_feature_1 = backbone(image_1_crop)\n",
    "        coarse_image_feature_2, fine_image_feature_2 = backbone(image_2_crop)\n",
    "        \n",
    "        coarse_image_feature_1 = positional_encoding(coarse_image_feature_1)\n",
    "        coarse_image_feature_2 = positional_encoding(coarse_image_feature_2)\n",
    "\n",
    "        coarse_image_feature_1 = rearrange(\n",
    "            coarse_image_feature_1, \"n c h w -> n (h w) c\"\n",
    "        )\n",
    "        coarse_image_feature_2 = rearrange(\n",
    "            coarse_image_feature_2, \"n c h w -> n (h w) c\"\n",
    "        )\n",
    "\n",
    "        coarse_image_feature_1, coarse_image_feature_2 = coarse_loftr(\n",
    "            coarse_image_feature_1, coarse_image_feature_2\n",
    "        )\n",
    "\n",
    "        coarse_matches = coarse_matcher(coarse_image_feature_1, coarse_image_feature_2)\n",
    "\n",
    "        coarse_matches_ground_truth = {\n",
    "            \"batch_indices\": match_matrix.nonzero()[:, 0],\n",
    "            \"row_indices\": match_matrix.nonzero()[:, 1],\n",
    "            \"column_indices\": match_matrix.nonzero()[:, 2],\n",
    "        }\n",
    "\n",
    "        fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_preprocess(\n",
    "            coarse_image_feature_1=coarse_image_feature_1,\n",
    "            coarse_image_feature_2=coarse_image_feature_2,\n",
    "            fine_image_feature_1=fine_image_feature_1,\n",
    "            fine_image_feature_2=fine_image_feature_2,\n",
    "            coarse_matches=coarse_matches_ground_truth,\n",
    "            fine_height_width=fine_height_width,\n",
    "            coarse_height_width=coarse_height_width,\n",
    "        )\n",
    "\n",
    "        fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_loftr(\n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "        )\n",
    "\n",
    "        predicted_relative_coordinates = fine_matching(\n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "        )\n",
    "\n",
    "        if coarse_loss == \"focal\":\n",
    "            coarse_loss_value = coarse_focal_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "\n",
    "        elif coarse_loss == \"official_focal\":\n",
    "            coarse_loss_value = coarse_official_focal_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "\n",
    "        elif coarse_loss == \"cross_entropy\":\n",
    "            coarse_loss_value = coarse_cross_entropy_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "            )\n",
    "\n",
    "        if fine_loss == \"l2\":\n",
    "            fine_loss_value = fine_l2_loss(\n",
    "                coordinates_predicted=predicted_relative_coordinates,\n",
    "                coordinates_ground_truth=relative_coordinates,\n",
    "            )\n",
    "\n",
    "        elif fine_loss == \"l2_std\":\n",
    "            fine_loss_value = fine_l2_loss_with_standard_deviation(\n",
    "                coordinates_predicted=predicted_relative_coordinates,\n",
    "                coordinates_ground_truth=relative_coordinates,\n",
    "            )\n",
    "\n",
    "        loss = coarse_loss_value + fine_loss_value\n",
    "        loss = loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        coarse_loss_history.append(coarse_loss_value.cpu().item())\n",
    "        fine_loss_history.append(fine_loss_value.cpu().item())\n",
    "    \n",
    "    if epoch % 10 ==0:\n",
    "        print(f\"Epoch: {epoch}, Seen Datapoints: {seen_datapoints}, Duration: {epoch_duration:.2f} sec, Coarse Loss: {coarse_loss_value:.4f}, Fine Loss: {fine_loss_value:.4f}\")\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        hyperparameters = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"coarse_context\": use_coarse_context,\n",
    "        \"img_size\": \"3344_3904\",\n",
    "        \"ResNet\": backbone.__class__.__name__,\n",
    "        \"affine_transformation_range\": affine_transformation_range,\n",
    "        \"perspective_transformation_range\": perspective_transformation_range,\n",
    "        \"temperature\": temperature,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"coarse_loss\": coarse_loss,\n",
    "        \"fine_loss\": fine_loss,\n",
    "        \"use_train_data\": use_train_data,\n",
    "        \"block_dimensions\": block_dimensions,\n",
    "        \"use_l2_with_standard_deviation\": use_l2_with_standard_deviation,\n",
    "        \"seen_datapoints\": seen_datapoints,\n",
    "        \"crop_size\": crop_size,\n",
    "        \"patch_size\": patch_size\n",
    "        }\n",
    "\n",
    "        if coarse_loss == \"focal\":\n",
    "            hyperparameters[\"gamma\"] = gamma\n",
    "            hyperparameters[\"alpha\"] = alpha\n",
    "\n",
    "        if scheduler:\n",
    "            hyperparameters[\"scheduler\"] = scheduler.__class__.__name__\n",
    "            hyperparameters[\"learning_rate_gamma\"] = learning_rate_gamma\n",
    "            hyperparameters[\"learning_rate_step_size\"] = learning_rate_step_size\n",
    "        \n",
    "        models = {\"backbone\": backbone, \"coarse_loftr\": coarse_loftr, \"fine_loftr\": fine_loftr}\n",
    "        model_directory = save_model(\n",
    "            models,\n",
    "            hyperparameters=hyperparameters,\n",
    "            coarse_loss_history=coarse_loss_history,\n",
    "            fine_loss_history=fine_loss_history,\n",
    "            base_path=f\"../../models/{training_run}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_hyperbrain",
   "language": "python",
   "name": "conda_hyperbrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
