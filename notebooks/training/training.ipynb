{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/p/home/jusers/pierschke1/shared/HyperBrain\")\n",
    "sys.path.append(\"/p/home/jusers/pierschke1/shared/HyperBrain\\\\source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from source.datasets.brain_dataset import BrainDataset\n",
    "from source.loftr.backbone import ResNetFPN_16_4, ResNetFPN_8_2\n",
    "from source.loftr.positional_encoding import PositionalEncoding\n",
    "from source.loftr.transformer import LocalFeatureTransformer\n",
    "from source.loftr.coarse_matching import CoarseMatching\n",
    "from source.loftr.fine_matching import FineMatching\n",
    "from source.loftr.fine_preprocess import FinePreprocess\n",
    "from source.loftr.loss import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from source.miscellaneous.evaluation import evaluate_model, read_model_evaluation_metrics\n",
    "from einops.einops import rearrange\n",
    "from source.miscellaneous.model_saving import save_model\n",
    "from source.datasets.brain_dataset import collate_fn\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crop_size = 480\n",
    "affine_transformation_range = 0.25\n",
    "perspective_transformation_range = 0.000125\n",
    "patch_size = 8\n",
    "max_translation_shift = 50\n",
    "fine_height_width = (crop_size//patch_size)*4\n",
    "coarse_height_width = crop_size//patch_size\n",
    "images_directory = \"../../data/cyto_downscaled_3344_3904/\"\n",
    "use_train_data = True\n",
    "\n",
    "dataset_train = BrainDataset(\n",
    "    images_directory=images_directory,\n",
    "    train=use_train_data,\n",
    "    affine_transformation_range=affine_transformation_range,\n",
    "    perspective_transformation_range=perspective_transformation_range,\n",
    "    crop_size=crop_size,\n",
    "    patch_size=patch_size,\n",
    "    max_translation_shift=max_translation_shift,\n",
    "    fine_height_width=fine_height_width,\n",
    "    transform=v2.Compose([v2.Normalize(mean=[0.594], std=[0.204])]),\n",
    "    load_in_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0015\n",
    "use_coarse_context = False\n",
    "use_l2_with_standard_deviation = False\n",
    "temperature = (\n",
    "    0.2  # Dont decrease this value, it will yield in overflows (similarity_matrix)\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# block_dimensions_8_2 = [96, 128, 192]\n",
    "# block_dimensions_16_4 = [64, 96, 128, 192]\n",
    "block_dimensions = [128, 196, 256]\n",
    "fine_feature_size = block_dimensions[0]  # 1 for 16_4, 0 for 8_2\n",
    "coarse_feature_size = block_dimensions[-1]\n",
    "backbone = ResNetFPN_8_2(block_dimensions=block_dimensions).to(device=device)\n",
    "# backbone = nn.DataParallel(backbone)\n",
    "\n",
    "positional_encoding = PositionalEncoding(coarse_feature_size).to(device=device)\n",
    "\n",
    "coarse_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=coarse_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"] * 4,\n",
    ").to(device=device)\n",
    "# coarse_loftr = nn.DataParallel(coarse_loftr)\n",
    "\n",
    "\n",
    "coarse_matcher = CoarseMatching(temperature=temperature, confidence_threshold=0.2).to(\n",
    "    device=device\n",
    ")\n",
    "\n",
    "fine_preprocess = FinePreprocess(\n",
    "    coarse_feature_size=coarse_feature_size,\n",
    "    fine_feature_size=fine_feature_size,\n",
    "    window_size=5,\n",
    "    use_coarse_context=use_coarse_context,\n",
    ").to(device=device)\n",
    "fine_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=fine_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"],\n",
    ").to(device=device)\n",
    "# fine_loftr = nn.DataParallel(coarse_loftr)\n",
    "\n",
    "\n",
    "fine_matching = FineMatching(\n",
    "    return_standard_deviation=use_l2_with_standard_deviation,\n",
    ").to(device=device)\n",
    "\n",
    "params = list(backbone.parameters()) + list(coarse_loftr.parameters()) + list(fine_loftr.parameters())\n",
    "optimizer = torch.optim.Adam(params, weight_decay=weight_decay, lr=learning_rate)\n",
    "\n",
    "learning_rate_gamma = 0.85\n",
    "learning_rate_step_size = 80\n",
    "scheduler = StepLR(optimizer, step_size=learning_rate_step_size, gamma=learning_rate_gamma)\n",
    "\n",
    "coarse_loss_history = []\n",
    "fine_loss_history = []\n",
    "loss_hist_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coarse_loss = \"official_focal\"\n",
    "alpha = 0.75\n",
    "gamma = 2\n",
    "fine_loss = \"l2_std\" if use_l2_with_standard_deviation else \"l2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seen_datapoints = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_run = \"run_x0004\"\n",
    "model_directory = \"../../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Seen Datapoints: 4080, Duration: 1.53 sec, Coarse Loss: 3.6246, Fine Loss: 0.4341\n",
      "Epoch: 20, Seen Datapoints: 4160, Duration: 1.48 sec, Coarse Loss: 3.3178, Fine Loss: 0.4179\n",
      "Latest AUC: 7.485180706726387\n",
      "Epoch: 30, Seen Datapoints: 4240, Duration: 1.48 sec, Coarse Loss: 2.9782, Fine Loss: 0.4017\n",
      "Epoch: 40, Seen Datapoints: 4320, Duration: 1.47 sec, Coarse Loss: 3.0517, Fine Loss: 0.3940\n",
      "Latest AUC: 7.543464165799087\n",
      "Warning: Crop position is invalid. Returning crop position as (0,0)\n",
      "Epoch: 50, Seen Datapoints: 4400, Duration: 1.45 sec, Coarse Loss: 2.9045, Fine Loss: 0.4611\n",
      "Epoch: 60, Seen Datapoints: 4480, Duration: 1.53 sec, Coarse Loss: 3.4575, Fine Loss: 0.4141\n",
      "Latest AUC: 7.483694479372353\n",
      "Epoch: 70, Seen Datapoints: 4560, Duration: 1.50 sec, Coarse Loss: 3.4193, Fine Loss: 0.4404\n",
      "Epoch: 80, Seen Datapoints: 4640, Duration: 1.41 sec, Coarse Loss: 3.0044, Fine Loss: 0.4036\n",
      "Latest AUC: 7.568233170359745\n",
      "Epoch: 90, Seen Datapoints: 4720, Duration: 1.46 sec, Coarse Loss: 5.6233, Fine Loss: 0.5681\n",
      "Epoch: 100, Seen Datapoints: 4800, Duration: 1.53 sec, Coarse Loss: 2.5945, Fine Loss: 0.3980\n",
      "Latest AUC: 7.487723441059935\n",
      "Epoch: 110, Seen Datapoints: 4880, Duration: 1.46 sec, Coarse Loss: 3.0425, Fine Loss: 0.4114\n",
      "Epoch: 120, Seen Datapoints: 4960, Duration: 1.39 sec, Coarse Loss: 3.2735, Fine Loss: 0.4905\n",
      "Latest AUC: 7.559673582472606\n",
      "Epoch: 130, Seen Datapoints: 5040, Duration: 1.49 sec, Coarse Loss: 3.2597, Fine Loss: 0.4819\n",
      "Epoch: 140, Seen Datapoints: 5120, Duration: 1.47 sec, Coarse Loss: 3.6145, Fine Loss: 0.4029\n",
      "Latest AUC: 7.556668887999257\n",
      "Epoch: 150, Seen Datapoints: 5200, Duration: 1.51 sec, Coarse Loss: 3.0447, Fine Loss: 0.3806\n",
      "Epoch: 160, Seen Datapoints: 5280, Duration: 1.45 sec, Coarse Loss: 4.9081, Fine Loss: 0.5137\n",
      "Latest AUC: 7.583764603908639\n",
      "Epoch: 170, Seen Datapoints: 5360, Duration: 1.49 sec, Coarse Loss: 2.2990, Fine Loss: 0.3602\n",
      "Epoch: 180, Seen Datapoints: 5440, Duration: 1.44 sec, Coarse Loss: 7.2789, Fine Loss: 0.5432\n",
      "Latest AUC: 7.554928613864469\n",
      "Epoch: 190, Seen Datapoints: 5520, Duration: 1.49 sec, Coarse Loss: 3.1721, Fine Loss: 0.4207\n",
      "Epoch: 200, Seen Datapoints: 5600, Duration: 1.49 sec, Coarse Loss: 2.6117, Fine Loss: 0.3726\n",
      "Latest AUC: 7.559475413691253\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for step, batch in enumerate(dataloader_train):\n",
    "        image_1_crop, image_2_crop, match_matrix, relative_coordinates, _ = batch\n",
    "\n",
    "        seen_datapoints += image_1_crop.shape[0]\n",
    "        \n",
    "        image_1_crop = image_1_crop.to(device=device)\n",
    "        image_2_crop = image_2_crop.to(device=device)\n",
    "        match_matrix = match_matrix.to(device=device)\n",
    "        relative_coordinates = relative_coordinates.to(device=device)\n",
    "\n",
    "        coarse_image_feature_1, fine_image_feature_1 = backbone(image_1_crop)\n",
    "        coarse_image_feature_2, fine_image_feature_2 = backbone(image_2_crop)\n",
    "        \n",
    "        coarse_image_feature_1 = positional_encoding(coarse_image_feature_1)\n",
    "        coarse_image_feature_2 = positional_encoding(coarse_image_feature_2)\n",
    "\n",
    "        coarse_image_feature_1 = rearrange(\n",
    "            coarse_image_feature_1, \"n c h w -> n (h w) c\"\n",
    "        )\n",
    "        coarse_image_feature_2 = rearrange(\n",
    "            coarse_image_feature_2, \"n c h w -> n (h w) c\"\n",
    "        )\n",
    "\n",
    "        coarse_image_feature_1, coarse_image_feature_2 = coarse_loftr(\n",
    "            coarse_image_feature_1, coarse_image_feature_2\n",
    "        )\n",
    "\n",
    "        coarse_matches = coarse_matcher(coarse_image_feature_1, coarse_image_feature_2)\n",
    "\n",
    "        coarse_matches_ground_truth = {\n",
    "            \"batch_indices\": match_matrix.nonzero()[:, 0],\n",
    "            \"row_indices\": match_matrix.nonzero()[:, 1],\n",
    "            \"column_indices\": match_matrix.nonzero()[:, 2],\n",
    "        }\n",
    "\n",
    "        fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_preprocess(\n",
    "            coarse_image_feature_1=coarse_image_feature_1,\n",
    "            coarse_image_feature_2=coarse_image_feature_2,\n",
    "            fine_image_feature_1=fine_image_feature_1,\n",
    "            fine_image_feature_2=fine_image_feature_2,\n",
    "            coarse_matches=coarse_matches_ground_truth,\n",
    "            fine_height_width=fine_height_width,\n",
    "            coarse_height_width=coarse_height_width,\n",
    "        )\n",
    "\n",
    "        fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_loftr(\n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "        )\n",
    "\n",
    "        predicted_relative_coordinates = fine_matching(\n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "        )\n",
    "\n",
    "        if coarse_loss == \"focal\":\n",
    "            coarse_loss_value = coarse_focal_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "\n",
    "        elif coarse_loss == \"official_focal\":\n",
    "            coarse_loss_value = coarse_official_focal_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "\n",
    "        elif coarse_loss == \"cross_entropy\":\n",
    "            coarse_loss_value = coarse_cross_entropy_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "            )\n",
    "\n",
    "        if fine_loss == \"l2\":\n",
    "            fine_loss_value = fine_l2_loss(\n",
    "                coordinates_predicted=predicted_relative_coordinates,\n",
    "                coordinates_ground_truth=relative_coordinates,\n",
    "            )\n",
    "\n",
    "        elif fine_loss == \"l2_std\":\n",
    "            fine_loss_value = fine_l2_loss_with_standard_deviation(\n",
    "                coordinates_predicted=predicted_relative_coordinates,\n",
    "                coordinates_ground_truth=relative_coordinates,\n",
    "            )\n",
    "\n",
    "        loss = coarse_loss_value + fine_loss_value\n",
    "        loss = loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        coarse_loss_history.append(coarse_loss_value.cpu().item())\n",
    "        fine_loss_history.append(fine_loss_value.cpu().item())\n",
    "    \n",
    "    if epoch % 10 ==0:\n",
    "        print(f\"Epoch: {epoch}, Seen Datapoints: {seen_datapoints}, Duration: {epoch_duration:.2f} sec, Coarse Loss: {coarse_loss_value:.4f}, Fine Loss: {fine_loss_value:.4f}\")\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        hyperparameters = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"coarse_context\": use_coarse_context,\n",
    "        \"img_size\": \"3344_3904\",\n",
    "        \"ResNet\": backbone.__class__.__name__,\n",
    "        \"affine_transformation_range\": affine_transformation_range,\n",
    "        \"perspective_transformation_range\": perspective_transformation_range,\n",
    "        \"temperature\": temperature,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"coarse_loss\": coarse_loss,\n",
    "        \"fine_loss\": fine_loss,\n",
    "        \"use_train_data\": use_train_data,\n",
    "        \"block_dimensions\": block_dimensions,\n",
    "        \"use_l2_with_standard_deviation\": use_l2_with_standard_deviation,\n",
    "        \"seen_datapoints\": seen_datapoints,\n",
    "        \"crop_size\": crop_size,\n",
    "        \"patch_size\": patch_size\n",
    "        }\n",
    "\n",
    "        if coarse_loss == \"focal\":\n",
    "            hyperparameters[\"gamma\"] = gamma\n",
    "            hyperparameters[\"alpha\"] = alpha\n",
    "\n",
    "        if scheduler:\n",
    "            hyperparameters[\"scheduler\"] = scheduler.__class__.__name__\n",
    "            hyperparameters[\"learning_rate_gamma\"] = learning_rate_gamma\n",
    "            hyperparameters[\"learning_rate_step_size\"] = learning_rate_step_size\n",
    "        \n",
    "        models = {\"backbone\": backbone, \"coarse_loftr\": coarse_loftr, \"fine_loftr\": fine_loftr}\n",
    "        model_directory = save_model(\n",
    "            models,\n",
    "            hyperparameters=hyperparameters,\n",
    "            coarse_loss_history=coarse_loss_history,\n",
    "            fine_loss_history=fine_loss_history,\n",
    "            base_path=f\"../../models/{training_run}\"\n",
    "        )\n",
    "\n",
    "        evaluation_metrics = evaluate_model(training_run=training_run, model_name=str(model_directory.split(\"/\")[-1]), confidence_threshold=0.8)\n",
    "        print(f\"Latest AUC: {evaluation_metrics['auc']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_hyperbrain",
   "language": "python",
   "name": "conda_hyperbrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
