{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/p/home/jusers/pierschke1/shared/HyperBrain\")\n",
    "sys.path.append(\"/p/home/jusers/pierschke1/shared/HyperBrain\\\\source\")\n",
    "sys.path.append(\"c:\\\\Users\\\\robin\\\\Documents\\\\HyperBrain\")\n",
    "sys.path.append(\"c:\\\\Users\\\\robin\\\\Documents\\\\HyperBrain\\\\source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from source.datasets.brain_dataset import BrainDataset\n",
    "from source.loftr.backbone import ResNetFPN_32_8, ResNetFPN_16_4, ResNetFPN_8_2\n",
    "from source.loftr.positional_encoding import PositionalEncoding\n",
    "from source.loftr.transformer import LocalFeatureTransformer\n",
    "from source.loftr.coarse_matching import CoarseMatching\n",
    "from source.loftr.fine_matching import FineMatching\n",
    "from source.loftr.fine_preprocess import FinePreprocess\n",
    "from source.loftr.loss import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from source.miscellaneous.evaluation import evaluate_model, read_model_evaluation_metrics\n",
    "from einops.einops import rearrange\n",
    "from source.miscellaneous.model_saving import save_model\n",
    "from source.datasets.brain_dataset import collate_fn\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crop_size = 1280\n",
    "affine_transformation_range = 0.1\n",
    "perspective_transformation_range = 0\n",
    "patch_size = 32\n",
    "max_translation_shift = 100\n",
    "fine_height_width = (crop_size//patch_size)*4\n",
    "coarse_height_width = crop_size//patch_size\n",
    "images_directory = \"../../data/cyto_downscaled_3344_3904/\"\n",
    "use_train_data = True\n",
    "attention = \"linear\"\n",
    "\n",
    "dataset_train = BrainDataset(\n",
    "    images_directory=images_directory,\n",
    "    train=use_train_data,\n",
    "    affine_transformation_range=affine_transformation_range,\n",
    "    perspective_transformation_range=perspective_transformation_range,\n",
    "    crop_size=crop_size,\n",
    "    patch_size=patch_size,\n",
    "    max_translation_shift=max_translation_shift,\n",
    "    fine_height_width=fine_height_width,\n",
    "    transform=v2.Compose([v2.Normalize(mean=[0.594], std=[0.204])]),\n",
    "    load_in_gpu=True,\n",
    "    sample_both_coordinates=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0015\n",
    "use_coarse_context = False\n",
    "use_l2_with_standard_deviation = False\n",
    "temperature = (\n",
    "    0.2  # Dont decrease this value, it will yield in overflows (similarity_matrix)\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# block_dimensions_8_2 =\n",
    "# block_dimensions_16_4 = [64, 96, 128, 192]\n",
    "# block_dimensions_32_8 = [32, 64, 96, 128, 192]\n",
    "block_dimensions = [32, 64, 96, 128, 192]\n",
    "fine_feature_size = block_dimensions[2]  # 2 for 32_8, 1 for 16_4, 0 for 8_2\n",
    "coarse_feature_size = block_dimensions[-1]\n",
    "backbone = ResNetFPN_32_8(block_dimensions=block_dimensions).to(device=device)\n",
    "backbone = nn.DataParallel(backbone)\n",
    "\n",
    "positional_encoding = PositionalEncoding(coarse_feature_size).to(device=device)\n",
    "\n",
    "coarse_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=coarse_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"] * 4,\n",
    "    attention_type=attention\n",
    ").to(device=device)\n",
    "coarse_loftr = nn.DataParallel(coarse_loftr)\n",
    "\n",
    "\n",
    "coarse_matcher = CoarseMatching(temperature=temperature, confidence_threshold=0.2).to(\n",
    "    device=device\n",
    ")\n",
    "\n",
    "fine_preprocess = FinePreprocess(\n",
    "    coarse_feature_size=coarse_feature_size,\n",
    "    fine_feature_size=fine_feature_size,\n",
    "    window_size=5,\n",
    "    use_coarse_context=use_coarse_context,\n",
    ").to(device=device)\n",
    "fine_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=fine_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"],\n",
    "    attention_type=attention\n",
    ").to(device=device)\n",
    "# fine_loftr = nn.DataParallel(coarse_loftr)\n",
    "\n",
    "\n",
    "fine_matching = FineMatching(\n",
    "    return_standard_deviation=use_l2_with_standard_deviation,\n",
    ").to(device=device)\n",
    "\n",
    "params = list(backbone.parameters()) + list(coarse_loftr.parameters()) + list(fine_loftr.parameters())\n",
    "optimizer = torch.optim.Adam(params, weight_decay=weight_decay, lr=learning_rate)\n",
    "\n",
    "learning_rate_gamma = 0.85\n",
    "learning_rate_step_size = 80\n",
    "scheduler = StepLR(optimizer, step_size=learning_rate_step_size, gamma=learning_rate_gamma)\n",
    "\n",
    "coarse_loss_history = []\n",
    "fine_loss_history = []\n",
    "loss_hist_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coarse_loss = \"official_focal\"\n",
    "alpha = 0.45\n",
    "gamma = 2\n",
    "fine_loss = \"l2_std\" if use_l2_with_standard_deviation else \"l2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seen_datapoints = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_run = \"coarse_matching\"\n",
    "model_directory = \"../../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='cuda:0', size=(0, 25, 96))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_image_feature_2_unfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(994)\n",
      "tensor(0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot reshape tensor of 0 elements into shape [0, -1, 8, 12] because the unspecified dimension size -1 can be any value and is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m\n\u001b[1;32m     36\u001b[0m coarse_matches_ground_truth \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m: match_matrix\u001b[38;5;241m.\u001b[39mnonzero()[:, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m: match_matrix\u001b[38;5;241m.\u001b[39mnonzero()[:, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m: match_matrix\u001b[38;5;241m.\u001b[39mnonzero()[:, \u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     40\u001b[0m }\n\u001b[1;32m     42\u001b[0m fine_image_feature_1_unfold, fine_image_feature_2_unfold \u001b[38;5;241m=\u001b[39m fine_preprocess(\n\u001b[1;32m     43\u001b[0m     coarse_image_feature_1\u001b[38;5;241m=\u001b[39mcoarse_image_feature_1,\n\u001b[1;32m     44\u001b[0m     coarse_image_feature_2\u001b[38;5;241m=\u001b[39mcoarse_image_feature_2,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     coarse_height_width\u001b[38;5;241m=\u001b[39mcoarse_height_width,\n\u001b[1;32m     50\u001b[0m )\n\u001b[0;32m---> 52\u001b[0m fine_image_feature_1_unfold, fine_image_feature_2_unfold \u001b[38;5;241m=\u001b[39m fine_loftr(\n\u001b[1;32m     53\u001b[0m     fine_image_feature_1_unfold, fine_image_feature_2_unfold\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m predicted_relative_coordinates \u001b[38;5;241m=\u001b[39m fine_matching(\n\u001b[1;32m     57\u001b[0m     fine_image_feature_1_unfold, fine_image_feature_2_unfold\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m coarse_loss \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfocal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/PROJECT_training2005/testdir/miniconda3/envs/hyperbrain/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/PROJECT_training2005/testdir/miniconda3/envs/hyperbrain/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/p/home/jusers/pierschke1/shared/HyperBrain/source/loftr/transformer.py:213\u001b[0m, in \u001b[0;36mLocalFeatureTransformer.forward\u001b[0;34m(self, feature_image_1, feature_image_2)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_names):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 213\u001b[0m         feature_image_1 \u001b[38;5;241m=\u001b[39m layer(feature_image_1, feature_image_1)\n\u001b[1;32m    214\u001b[0m         feature_image_2 \u001b[38;5;241m=\u001b[39m layer(feature_image_2, feature_image_2)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/PROJECT_training2005/testdir/miniconda3/envs/hyperbrain/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/PROJECT_training2005/testdir/miniconda3/envs/hyperbrain/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/p/home/jusers/pierschke1/shared/HyperBrain/source/loftr/transformer.py:111\u001b[0m, in \u001b[0;36mLoFTREncoderLayer.forward\u001b[0;34m(self, query_tensor, context_tensor)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# multi-head attention\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# The following view operations split the query, key and value tensors into multiple heads.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# eg an 192 dimensional feature vector could be split into 8 heads of 24 dimensions each.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_projection(query)\n\u001b[0;32m--> 111\u001b[0m query \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    112\u001b[0m     batchsize, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dimension\n\u001b[1;32m    113\u001b[0m )  \u001b[38;5;66;03m# [Batchsize, L, Number of heads, Head dimension])]\u001b[39;00m\n\u001b[1;32m    115\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_projection(key)\n\u001b[1;32m    116\u001b[0m key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    117\u001b[0m     batchsize, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dimension\n\u001b[1;32m    118\u001b[0m )  \u001b[38;5;66;03m# [Batchsize, S, Number of heads, Head dimension]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot reshape tensor of 0 elements into shape [0, -1, 8, 12] because the unspecified dimension size -1 can be any value and is ambiguous"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for step, batch in enumerate(dataloader_train):\n",
    "        image_1_crop, image_2_crop, match_matrix, relative_coordinates, _ = batch\n",
    "        number_of_matches = match_matrix.sum()\n",
    "        print(number_of_matches)\n",
    "\n",
    "        seen_datapoints += image_1_crop.shape[0]\n",
    "        \n",
    "        image_1_crop = image_1_crop.to(device=device)\n",
    "        image_2_crop = image_2_crop.to(device=device)\n",
    "        match_matrix = match_matrix.to(device=device)\n",
    "        relative_coordinates = relative_coordinates.to(device=device)\n",
    "\n",
    "        coarse_image_feature_1, fine_image_feature_1 = backbone(image_1_crop)\n",
    "        coarse_image_feature_2, fine_image_feature_2 = backbone(image_2_crop)\n",
    "        \n",
    "        coarse_image_feature_1 = positional_encoding(coarse_image_feature_1)\n",
    "        coarse_image_feature_2 = positional_encoding(coarse_image_feature_2)\n",
    "\n",
    "        coarse_image_feature_1 = rearrange(\n",
    "            coarse_image_feature_1, \"n c h w -> n (h w) c\"\n",
    "        )\n",
    "        coarse_image_feature_2 = rearrange(\n",
    "            coarse_image_feature_2, \"n c h w -> n (h w) c\"\n",
    "        )\n",
    "\n",
    "        coarse_image_feature_1, coarse_image_feature_2 = coarse_loftr(\n",
    "            coarse_image_feature_1, coarse_image_feature_2\n",
    "        )\n",
    "\n",
    "        coarse_matches = coarse_matcher(coarse_image_feature_1, coarse_image_feature_2)\n",
    "\n",
    "        coarse_matches_ground_truth = {\n",
    "            \"batch_indices\": match_matrix.nonzero()[:, 0],\n",
    "            \"row_indices\": match_matrix.nonzero()[:, 1],\n",
    "            \"column_indices\": match_matrix.nonzero()[:, 2],\n",
    "        }\n",
    "        \n",
    "        if number_of_matches!=0:  #TODO double check if always same number of matches in batch\n",
    "            \n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_preprocess(\n",
    "                coarse_image_feature_1=coarse_image_feature_1,\n",
    "                coarse_image_feature_2=coarse_image_feature_2,\n",
    "                fine_image_feature_1=fine_image_feature_1,\n",
    "                fine_image_feature_2=fine_image_feature_2,\n",
    "                coarse_matches=coarse_matches_ground_truth,\n",
    "                fine_height_width=fine_height_width,\n",
    "                coarse_height_width=coarse_height_width,\n",
    "            )\n",
    "\n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_loftr(\n",
    "                fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "            )\n",
    "\n",
    "            predicted_relative_coordinates = fine_matching(\n",
    "                fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "            )\n",
    "\n",
    "        if coarse_loss == \"focal\":\n",
    "            coarse_loss_value = coarse_focal_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "\n",
    "        elif coarse_loss == \"official_focal\":\n",
    "            coarse_loss_value = coarse_official_focal_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "\n",
    "        elif coarse_loss == \"cross_entropy\":\n",
    "            coarse_loss_value = coarse_cross_entropy_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "            )\n",
    "\n",
    "        if fine_loss == \"l2\" and number_of_matches != 0:\n",
    "            fine_loss_value = fine_l2_loss(\n",
    "                coordinates_predicted=predicted_relative_coordinates,\n",
    "                coordinates_ground_truth=relative_coordinates,\n",
    "            )\n",
    "\n",
    "        elif fine_loss == \"l2_std\" and number_of_matches != 0:\n",
    "            fine_loss_value = fine_l2_loss_with_standard_deviation(\n",
    "                coordinates_predicted=predicted_relative_coordinates,\n",
    "                coordinates_ground_truth=relative_coordinates,\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            fine_loss_value = 0\n",
    "\n",
    "        loss = coarse_loss_value + fine_loss_value\n",
    "        loss = loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        coarse_loss_history.append(coarse_loss_value.cpu().item())\n",
    "        fine_loss_history.append(fine_loss_value.cpu().item())\n",
    "    \n",
    "    if epoch % 10 ==1:\n",
    "        print(f\"Epoch: {epoch}, Seen Datapoints: {seen_datapoints}, Duration: {epoch_duration:.2f} sec, Coarse Loss: {coarse_loss_value:.4f}, Fine Loss: {fine_loss_value:.4f}\")\n",
    "\n",
    "    if epoch % 20 == 1 and epoch > 1:\n",
    "        hyperparameters = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"coarse_context\": use_coarse_context,\n",
    "        \"img_size\": \"3344_3904\",\n",
    "        \"ResNet\": backbone.__class__.__name__,\n",
    "        \"affine_transformation_range\": affine_transformation_range,\n",
    "        \"perspective_transformation_range\": perspective_transformation_range,\n",
    "        \"temperature\": temperature,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"coarse_loss\": coarse_loss,\n",
    "        \"fine_loss\": fine_loss,\n",
    "        \"use_train_data\": use_train_data,\n",
    "        \"block_dimensions\": block_dimensions,\n",
    "        \"use_l2_with_standard_deviation\": use_l2_with_standard_deviation,\n",
    "        \"seen_datapoints\": seen_datapoints,\n",
    "        \"crop_size\": crop_size,\n",
    "        \"patch_size\": patch_size\n",
    "        }\n",
    "\n",
    "        if coarse_loss == \"focal\":\n",
    "            hyperparameters[\"gamma\"] = gamma\n",
    "            hyperparameters[\"alpha\"] = alpha\n",
    "\n",
    "        if scheduler:\n",
    "            hyperparameters[\"scheduler\"] = scheduler.__class__.__name__\n",
    "            hyperparameters[\"learning_rate_gamma\"] = learning_rate_gamma\n",
    "            hyperparameters[\"learning_rate_step_size\"] = learning_rate_step_size\n",
    "        \n",
    "        models = {\"backbone\": backbone, \"coarse_loftr\": coarse_loftr, \"fine_loftr\": fine_loftr}\n",
    "        model_directory = save_model(\n",
    "            models,\n",
    "            hyperparameters=hyperparameters,\n",
    "            coarse_loss_history=coarse_loss_history,\n",
    "            fine_loss_history=fine_loss_history,\n",
    "            base_path=f\"../../models/{training_run}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_hyperbrain",
   "language": "python",
   "name": "conda_hyperbrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
