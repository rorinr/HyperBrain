{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/p/home/jusers/pierschke1/shared/HyperBrain\")\n",
    "sys.path.append(\"/p/home/jusers/pierschke1/shared/HyperBrain\\\\source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from source.datasets.brain_dataset import BrainDataset\n",
    "from source.loftr.backbone import ResNetFPN_16_4, ResNetFPN_8_2\n",
    "from source.loftr.positional_encoding import PositionalEncoding\n",
    "from source.loftr.transformer import LocalFeatureTransformer\n",
    "from source.loftr.coarse_matching import CoarseMatching\n",
    "from source.loftr.fine_matching import FineMatching\n",
    "from source.loftr.fine_preprocess import FinePreprocess\n",
    "from source.loftr.loss import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from source.miscellaneous.evaluation import evaluate_model, read_model_evaluation_metrics\n",
    "from einops.einops import rearrange\n",
    "from source.miscellaneous.model_saving import save_model\n",
    "from source.datasets.brain_dataset import collate_fn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crop_size = 480\n",
    "affine_transformation_range = 0.25\n",
    "perspective_transformation_range = 0.000125\n",
    "patch_size = 8\n",
    "max_translation_shift = 50\n",
    "fine_height_width = (crop_size//patch_size)*4\n",
    "coarse_height_width = crop_size//patch_size\n",
    "images_directory = \"../../data/cyto_downscaled_3344_3904/\"\n",
    "use_train_data = True\n",
    "self_supervised=False\n",
    "\n",
    "dataset_train = BrainDataset(\n",
    "    images_directory=images_directory,\n",
    "    train=use_train_data,\n",
    "    affine_transformation_range=affine_transformation_range,\n",
    "    perspective_transformation_range=perspective_transformation_range,\n",
    "    crop_size=crop_size,\n",
    "    patch_size=patch_size,\n",
    "    max_translation_shift=max_translation_shift,\n",
    "    fine_height_width=fine_height_width,\n",
    "    transform=v2.Compose([v2.Normalize(mean=[0.594], std=[0.204])]),\n",
    "    load_in_gpu=True,\n",
    "    self_supervised=self_supervised\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0015\n",
    "use_coarse_context = False\n",
    "use_l2_with_standard_deviation = False\n",
    "attention = \"linear\"\n",
    "temperature = (\n",
    "    0.2  # Dont decrease this value, it will yield in overflows (similarity_matrix)\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# block_dimensions_8_2 = [96, 128, 192]\n",
    "# block_dimensions_16_4 = [64, 96, 128, 192]\n",
    "block_dimensions =  [128, 196, 256]\n",
    "fine_feature_size = block_dimensions[0]  # 1 for 16_4, 0 for 8_2\n",
    "coarse_feature_size = block_dimensions[-1]\n",
    "backbone = ResNetFPN_8_2(block_dimensions=block_dimensions).to(device=device)\n",
    "backbone = nn.DataParallel(backbone)\n",
    "\n",
    "positional_encoding = PositionalEncoding(coarse_feature_size).to(device=device)\n",
    "\n",
    "coarse_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=coarse_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"] * 4,\n",
    "    attention_type=attention\n",
    ").to(device=device)\n",
    "coarse_loftr = nn.DataParallel(coarse_loftr)\n",
    "\n",
    "\n",
    "coarse_matcher = CoarseMatching(temperature=temperature, confidence_threshold=0.2).to(\n",
    "    device=device\n",
    ")\n",
    "\n",
    "fine_preprocess = FinePreprocess(\n",
    "    coarse_feature_size=coarse_feature_size,\n",
    "    fine_feature_size=fine_feature_size,\n",
    "    window_size=5,\n",
    "    use_coarse_context=use_coarse_context,\n",
    ").to(device=device)\n",
    "\n",
    "fine_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=fine_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"],\n",
    "    attention_type=attention\n",
    ").to(device=device)\n",
    "# fine_loftr = nn.DataParallel(coarse_loftr)\n",
    "\n",
    "\n",
    "fine_matching = FineMatching(\n",
    "    return_standard_deviation=use_l2_with_standard_deviation,\n",
    ").to(device=device)\n",
    "\n",
    "params = list(backbone.parameters()) + list(coarse_loftr.parameters()) + list(fine_loftr.parameters())\n",
    "optimizer = torch.optim.Adam(params, weight_decay=weight_decay, lr=learning_rate)\n",
    "\n",
    "learning_rate_gamma = 0.85\n",
    "learning_rate_step_size = 80\n",
    "scheduler = StepLR(optimizer, step_size=learning_rate_step_size, gamma=learning_rate_gamma)\n",
    "\n",
    "coarse_loss_history = []\n",
    "fine_loss_history = []\n",
    "loss_hist_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coarse_loss = \"official_focal\"\n",
    "alpha = 0.75 #TODO 0.45,.55,.65,.85\n",
    "gamma = 2.5 #todo 2\n",
    "fine_loss = \"l2_std\" if use_l2_with_standard_deviation else \"l2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seen_datapoints = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_run = \"run_x0012\"\n",
    "model_directory = \"../../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 103\u001b[0m\n\u001b[1;32m     99\u001b[0m     epoch_duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m epoch_start_time\n\u001b[1;32m    101\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 103\u001b[0m     coarse_loss_history\u001b[38;5;241m.\u001b[39mappend(coarse_loss_value\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    104\u001b[0m     fine_loss_history\u001b[38;5;241m.\u001b[39mappend(fine_loss_value\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 600\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for step, batch in enumerate(dataloader_train):\n",
    "        image_1_crop, image_2_crop, match_matrix, relative_coordinates, _ = batch\n",
    "\n",
    "        seen_datapoints += image_1_crop.shape[0]\n",
    "        \n",
    "        image_1_crop = image_1_crop.to(device=device)\n",
    "        image_2_crop = image_2_crop.to(device=device)\n",
    "        match_matrix = match_matrix.to(device=device)\n",
    "        relative_coordinates = relative_coordinates.to(device=device)\n",
    "\n",
    "        coarse_image_feature_1, fine_image_feature_1 = backbone(image_1_crop)\n",
    "        coarse_image_feature_2, fine_image_feature_2 = backbone(image_2_crop)\n",
    "        \n",
    "        coarse_image_feature_1 = positional_encoding(coarse_image_feature_1)\n",
    "        coarse_image_feature_2 = positional_encoding(coarse_image_feature_2)\n",
    "\n",
    "        coarse_image_feature_1 = rearrange(\n",
    "            coarse_image_feature_1, \"n c h w -> n (h w) c\"\n",
    "        )\n",
    "        coarse_image_feature_2 = rearrange(\n",
    "            coarse_image_feature_2, \"n c h w -> n (h w) c\"\n",
    "        )\n",
    "\n",
    "        coarse_image_feature_1, coarse_image_feature_2 = coarse_loftr(\n",
    "            coarse_image_feature_1, coarse_image_feature_2\n",
    "        )\n",
    "\n",
    "        coarse_matches = coarse_matcher(coarse_image_feature_1, coarse_image_feature_2)\n",
    "\n",
    "        coarse_matches_ground_truth = {\n",
    "            \"batch_indices\": match_matrix.nonzero()[:, 0],\n",
    "            \"row_indices\": match_matrix.nonzero()[:, 1],\n",
    "            \"column_indices\": match_matrix.nonzero()[:, 2],\n",
    "        }\n",
    "\n",
    "        fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_preprocess(\n",
    "            coarse_image_feature_1=coarse_image_feature_1,\n",
    "            coarse_image_feature_2=coarse_image_feature_2,\n",
    "            fine_image_feature_1=fine_image_feature_1,\n",
    "            fine_image_feature_2=fine_image_feature_2,\n",
    "            coarse_matches=coarse_matches_ground_truth,\n",
    "            fine_height_width=fine_height_width,\n",
    "            coarse_height_width=coarse_height_width,\n",
    "        )\n",
    "\n",
    "        fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_loftr(\n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "        )\n",
    "\n",
    "        predicted_relative_coordinates = fine_matching(\n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "        )\n",
    "\n",
    "        if coarse_loss == \"focal\":\n",
    "            coarse_loss_value = coarse_focal_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "\n",
    "        elif coarse_loss == \"official_focal\":\n",
    "            coarse_loss_value = coarse_official_focal_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "\n",
    "        elif coarse_loss == \"cross_entropy\":\n",
    "            coarse_loss_value = coarse_cross_entropy_loss(\n",
    "                predicted_confidence=coarse_matches[\"confidence_matrix\"],\n",
    "                ground_truth_confidence=match_matrix,\n",
    "            )\n",
    "\n",
    "        if fine_loss == \"l2\":\n",
    "            fine_loss_value = fine_l2_loss(\n",
    "                coordinates_predicted=predicted_relative_coordinates,\n",
    "                coordinates_ground_truth=relative_coordinates,\n",
    "            )\n",
    "\n",
    "        elif fine_loss == \"l2_std\":\n",
    "            fine_loss_value = fine_l2_loss_with_standard_deviation(\n",
    "                coordinates_predicted=predicted_relative_coordinates,\n",
    "                coordinates_ground_truth=relative_coordinates,\n",
    "            )\n",
    "\n",
    "        loss = coarse_loss_value + fine_loss_value\n",
    "        loss = loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        coarse_loss_history.append(coarse_loss_value.cpu().item())\n",
    "        fine_loss_history.append(fine_loss_value.cpu().item())\n",
    "    \n",
    "    if epoch % 10 ==0:\n",
    "        print(f\"Epoch: {epoch}, Seen Datapoints: {seen_datapoints}, Duration: {epoch_duration:.2f} sec, Coarse Loss: {coarse_loss_value:.4f}, Fine Loss: {fine_loss_value:.4f}\")\n",
    "\n",
    "    if epoch % 20 ==1:\n",
    "        hyperparameters = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"coarse_context\": use_coarse_context,\n",
    "        \"img_size\": \"3344_3904\",\n",
    "        \"ResNet\": backbone.__class__.__name__,\n",
    "        \"affine_transformation_range\": affine_transformation_range,\n",
    "        \"perspective_transformation_range\": perspective_transformation_range,\n",
    "        \"temperature\": temperature,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"coarse_loss\": coarse_loss,\n",
    "        \"fine_loss\": fine_loss,\n",
    "        \"use_train_data\": use_train_data,\n",
    "        \"block_dimensions\": block_dimensions,\n",
    "        \"use_l2_with_standard_deviation\": use_l2_with_standard_deviation,\n",
    "        \"seen_datapoints\": seen_datapoints,\n",
    "        \"crop_size\": crop_size,\n",
    "        \"patch_size\": patch_size,\n",
    "        \"attention\": attention,\n",
    "        \"selfsupervised\": self_supervised\n",
    "        }\n",
    "\n",
    "        if coarse_loss == \"official_focal\" or \"loftr_focal\":\n",
    "            hyperparameters[\"gamma\"] = gamma\n",
    "            hyperparameters[\"alpha\"] = alpha\n",
    "\n",
    "        if scheduler:\n",
    "            hyperparameters[\"scheduler\"] = scheduler.__class__.__name__\n",
    "            hyperparameters[\"learning_rate_gamma\"] = learning_rate_gamma\n",
    "            hyperparameters[\"learning_rate_step_size\"] = learning_rate_step_size\n",
    "        \n",
    "        models = {\"backbone\": backbone, \"coarse_loftr\": coarse_loftr, \"fine_loftr\": fine_loftr}\n",
    "        model_directory = save_model(\n",
    "            models,\n",
    "            hyperparameters=hyperparameters,\n",
    "            coarse_loss_history=coarse_loss_history,\n",
    "            fine_loss_history=fine_loss_history,\n",
    "            base_path=f\"../../models/{training_run}\"\n",
    "        )\n",
    "\n",
    "#         evaluation_metrics = evaluate_model(training_run=training_run, model_name=str(model_directory.split(\"/\")[-1]), confidence_threshold=0.8)\n",
    "#         print(f\"Latest AUC [{str(model_directory.split('/')[-1])}]: {evaluation_metrics['auc']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_hyperbrain",
   "language": "python",
   "name": "conda_hyperbrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
