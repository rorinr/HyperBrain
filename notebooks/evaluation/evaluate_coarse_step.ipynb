{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6547fe0b-99f7-4711-beb0-2bd252c9ec72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/p/home/jusers/pierschke1/shared/HyperBrain\")\n",
    "sys.path.append(\"/p/home/jusers/pierschke1/shared/HyperBrain\\\\source\")\n",
    "sys.path.append(\"c:\\\\Users\\\\robin\\\\Documents\\\\HyperBrain\")\n",
    "sys.path.append(\"c:\\\\Users\\\\robin\\\\Documents\\\\HyperBrain\\\\source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "581b4421-e991-401e-885a-cd98565f5462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from source.datasets.brain_dataset import BrainDataset\n",
    "from source.loftr.backbone import ResNetFPN_32_8\n",
    "from source.loftr.positional_encoding import PositionalEncoding\n",
    "from source.loftr.transformer import LocalFeatureTransformer\n",
    "from source.loftr.coarse_matching import CoarseMatching\n",
    "from source.loftr.fine_matching import FineMatching\n",
    "from source.loftr.fine_preprocess import FinePreprocess\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from einops.einops import rearrange\n",
    "from torchvision.transforms import v2\n",
    "from source.data_processing.image_reading import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from source.data_processing.keypoints import translate_patch_midpoints_and_refine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07697442-6401-474d-bb18-0283017e1e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patch_size = 32\n",
    "confidence_threshold = 0.9\n",
    "attention_type = \"linear\"\n",
    "model_name = \"5\"\n",
    "block_dimensions = [32, 64, 96, 128, 192]\n",
    "fine_feature_size = block_dimensions[2]\n",
    "coarse_feature_size = block_dimensions[-1]\n",
    "backbone = ResNetFPN_32_8(block_dimensions=block_dimensions).cuda()\n",
    "state_dict_backbone = torch.load(f\"../../models/coarse_matching/{model_name}/backbone.pt\")\n",
    "state_dict_backbone = {k.replace(\"module.\", \"\"): v for k, v in state_dict_backbone.items()}\n",
    "backbone.load_state_dict(state_dict_backbone)\n",
    "\n",
    "positional_encoding = PositionalEncoding(coarse_feature_size).cuda()\n",
    "\n",
    "coarse_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=coarse_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"] * 4,\n",
    "    attention_type=attention_type\n",
    ").cuda()\n",
    "state_dict_coarse_loftr = torch.load(f\"../../models/coarse_matching/{model_name}/coarse_loftr.pt\")\n",
    "state_dict_coarse_loftr = {k.replace(\"module.\", \"\"): v for k, v in state_dict_coarse_loftr.items()}\n",
    "coarse_loftr.load_state_dict(state_dict_coarse_loftr)\n",
    "\n",
    "coarse_matcher = CoarseMatching(\n",
    "    temperature=0.1, confidence_threshold=confidence_threshold\n",
    ").cuda()\n",
    "\n",
    "fine_preprocess = FinePreprocess(\n",
    "    coarse_feature_size=coarse_feature_size,\n",
    "    fine_feature_size=fine_feature_size,\n",
    "    window_size=5,\n",
    "    use_coarse_context=False,\n",
    ").cuda()\n",
    "\n",
    "fine_loftr = LocalFeatureTransformer(\n",
    "    feature_dimension=fine_feature_size,\n",
    "    number_of_heads=8,\n",
    "    layer_names=[\"self\", \"cross\"],\n",
    "    attention_type=attention_type\n",
    ").cuda()\n",
    "state_dict_fine_loftr = torch.load(f\"../../models/coarse_matching/{model_name}/fine_loftr.pt\")\n",
    "state_dict_fine_loftr = {k.replace(\"module.\", \"\"): v for k, v in state_dict_fine_loftr.items()}\n",
    "fine_loftr.load_state_dict(state_dict_fine_loftr)\n",
    "\n",
    "fine_matching = FineMatching().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "39987963-23e6-4210-9f3b-81dfe1f86c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_1 = read_image(\"../../data/cyto_downscaled_3344_3904_evaluation/B20_0524_Slice15.tif\")\n",
    "image_2 = read_image(\"../../data/cyto_downscaled_3344_3904_evaluation/B20_0525_Slice15.tif\")\n",
    "image_1, image_2 = ToTensor()(image_1), ToTensor()(image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2cce38fe-c9e0-4519-aead-38699191790b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8000, 3462]), torch.Size([1, 7382, 3668]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_1.size(), image_2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "716d087b-cae8-41dd-bf40-93455fddbee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crop_size = 1280\n",
    "image_1_x_num_windows = np.ceil(image_1.shape[2]/crop_size)\n",
    "image_1_x_step_size = (image_1.shape[2] - crop_size) / (image_1_x_num_windows - 1)\n",
    "\n",
    "image_1_y_num_windows = np.ceil(image_1.shape[1]/crop_size)\n",
    "image_1_y_step_size = (image_1.shape[1] - crop_size) / (image_1_y_num_windows - 1)\n",
    "\n",
    "image_2_x_num_windows = np.ceil(image_2.shape[2]/crop_size)\n",
    "image_2_x_step_size = (image_2.shape[2] - crop_size) / (image_2_x_num_windows - 1)\n",
    "\n",
    "image_2_y_num_windows = np.ceil(image_2.shape[1]/crop_size)\n",
    "image_2_y_step_size = (image_2.shape[1] - crop_size) / (image_2_y_num_windows - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c38ac9b4-2bc4-4d35-aa11-70cec852ed1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_1_crops = {}\n",
    "image_2_crops = {}\n",
    "\n",
    "for x in range(0, int(image_1_x_num_windows)-1):    \n",
    "    for y in range(0, int(image_1_y_num_windows)-1):\n",
    "        image_1_crops[f\"{x*crop_size}, {y*crop_size}\"] = image_1[:, y:y+crop_size, x:x+crop_size]\n",
    "        \n",
    "for x in range(0, int(image_2_x_num_windows)):    \n",
    "    for y in range(0, int(image_2_y_num_windows)):\n",
    "        image_2_crops[f\"{x*crop_size}, {y*crop_size}\"] = image_2[:, y:y+crop_size, x:x+crop_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e4d7a388-b457-45a3-9f12-41f8edf59992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matches_image_1 = []\n",
    "matches_image_2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for coordinates_1, crop_1 in image_1_crops.items():\n",
    "        for coordinates_2, crop_2 in image_2_crops.items():\n",
    "\n",
    "            coarse_image_feature_1, fine_image_feature_1 = backbone(crop_1.cuda().unsqueeze(0))\n",
    "            coarse_image_feature_2, fine_image_feature_2 = backbone(crop_2.cuda().unsqueeze(0))\n",
    "\n",
    "            fine_height_width = fine_image_feature_1.shape[-1]\n",
    "            coarse_height_width = coarse_image_feature_1.shape[-1]\n",
    "\n",
    "            coarse_image_feature_1 = positional_encoding(coarse_image_feature_1)\n",
    "            coarse_image_feature_2 = positional_encoding(coarse_image_feature_2)\n",
    "\n",
    "            coarse_image_feature_1 = rearrange(\n",
    "                coarse_image_feature_1, \"n c h w -> n (h w) c\"\n",
    "            )\n",
    "            coarse_image_feature_2 = rearrange(\n",
    "                coarse_image_feature_2, \"n c h w -> n (h w) c\"\n",
    "            )\n",
    "\n",
    "            coarse_image_feature_1, coarse_image_feature_2 = coarse_loftr(\n",
    "                coarse_image_feature_1, coarse_image_feature_2\n",
    "            )\n",
    "\n",
    "            coarse_matches_predicted = coarse_matcher(\n",
    "                coarse_image_feature_1, coarse_image_feature_2\n",
    "            )\n",
    "            match_matrix_predicted = coarse_matches_predicted[\"match_matrix\"]\n",
    "\n",
    "            (\n",
    "                fine_image_feature_1_unfold,\n",
    "                fine_image_feature_2_unfold,\n",
    "            ) = fine_preprocess(\n",
    "                coarse_image_feature_1=coarse_image_feature_1,\n",
    "                coarse_image_feature_2=coarse_image_feature_2,\n",
    "                fine_image_feature_1=fine_image_feature_1,\n",
    "                fine_image_feature_2=fine_image_feature_2,\n",
    "                coarse_matches=coarse_matches_predicted,\n",
    "                fine_height_width=fine_height_width,\n",
    "                coarse_height_width=coarse_height_width\n",
    "            )\n",
    "\n",
    "            # Skip crops that do not contain any matches\n",
    "            if fine_image_feature_1_unfold.size(0) == 0:\n",
    "                continue\n",
    "\n",
    "            fine_image_feature_1_unfold = fine_image_feature_1_unfold.to(\"cuda\")\n",
    "            fine_image_feature_2_unfold = fine_image_feature_2_unfold.to(\"cuda\")\n",
    "\n",
    "\n",
    "            fine_image_feature_1_unfold, fine_image_feature_2_unfold = fine_loftr(fine_image_feature_1_unfold, fine_image_feature_2_unfold)\n",
    "\n",
    "\n",
    "\n",
    "            predicted_relative_coordinates = fine_matching(\n",
    "                fine_image_feature_1_unfold, fine_image_feature_2_unfold\n",
    "            )\n",
    "\n",
    "            match_matrix_predicted = match_matrix_predicted.cpu()\n",
    "            predicted_relative_coordinates = predicted_relative_coordinates.cpu()\n",
    "\n",
    "            (\n",
    "                crop_1_patch_mid_coordinates,\n",
    "                crop_2_patch_mid_coordinates,\n",
    "                crop_2_patch_mid_coordinates_refined,\n",
    "            ) = translate_patch_midpoints_and_refine(\n",
    "                match_matrix=match_matrix_predicted,\n",
    "                patch_size=patch_size,\n",
    "                relative_coordinates=predicted_relative_coordinates,\n",
    "                fine_feature_size=fine_image_feature_1.shape[-1]\n",
    "            )\n",
    "\n",
    "            crop_1_patch_mid_coordinates = crop_1_patch_mid_coordinates.float()\n",
    "            crop_1_patch_mid_coordinates += torch.Tensor([int(i) for i in coordinates_1.split(\",\")])\n",
    "            crop_2_patch_mid_coordinates_refined += torch.Tensor([int(i) for i in coordinates_2.split(\",\")])\n",
    "\n",
    "            matches_image_1.append(crop_1_patch_mid_coordinates)\n",
    "            matches_image_2.append(crop_2_patch_mid_coordinates_refined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "efdd090c-7eb1-4aba-a00b-e5b411825849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matches_image_1 = torch.concat(matches_image_1)\n",
    "matches_image_2 = torch.concat(matches_image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fc527348-d873-4334-9bdf-fbbea06ce208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from source.miscellaneous.evaluation import read_deformation, evaluate_test_image_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca503a8c-afcd-4779-ae64-2fe97e188634",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- fix read_deformation function\n",
    "- make sure that adding the coordinates is correct: Why are there to large coordinates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c254744b-f2af-47bb-96fa-7fd93a7e0786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_deformation() -> torch.Tensor:\n",
    "    # Read deformation\n",
    "    deformation_path = (\n",
    "        r\"../../data/cyto_downscaled_3344_3904_evaluation/deformation.pt\"\n",
    "    )\n",
    "    deformation_file = h5py.File(deformation_path, \"r\")\n",
    "    deformation = torch.Tensor(np.array(deformation_file[\"deformation\"]) / 10)\n",
    "    deformation = kornia.augmentation.Resize(size=(3463, 8000), resample=\"NEAREST\")(deformation.permute(2,1,0))\n",
    "    deformation = deformation.permute(0, 3, 2, 1).squeeze(0)\n",
    "    deformation = torch.flip(deformation, dims=[-1])\n",
    "\n",
    "    return deformation.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e947abc9-501d-4877-b864-0bd00aa7b4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/home/jusers/pierschke1/shared/HyperBrain/source/miscellaneous/evaluation.py:93: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025842427/work/aten/src/ATen/native/BucketizationUtils.h:32.)\n",
      "  x_indices = torch.searchsorted(x_borders, x_coords) - 1\n"
     ]
    }
   ],
   "source": [
    "deformation = torch.load(r\"../../data/cyto_downscaled_3344_3904_evaluation/deformation.pt\")\n",
    "(\n",
    "    number_of_matches,\n",
    "    average_distance,\n",
    "    match_precision,\n",
    "    auc,\n",
    "    matches_per_patch,\n",
    "    entropy,\n",
    ") = evaluate_test_image_pair(matches_image_1, matches_image_2, deformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4b2c86e7-554c-4201-a0a4-581ce162570d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 3463, 2])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deformation.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_hyperbrain",
   "language": "python",
   "name": "conda_hyperbrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
