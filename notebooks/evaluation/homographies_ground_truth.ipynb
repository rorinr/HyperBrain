{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"c:\\\\Users\\\\robin\\\\Documents\\\\HyperBrain\")\n",
    "sys.path.append(\"c:\\\\Users\\\\robin\\\\Documents\\\\HyperBrain\\\\source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.data_processing.image_reading import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from source.miscellaneous.evaluation import load_deformation\n",
    "from source.data_processing.cropping import crop_image, create_crop_coordinate_mapping\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from kornia.geometry.homography import find_homography_dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test images\n",
    "image_1 = read_image(\n",
    "    r\"../../data/cyto_downscaled_3344_3904_evaluation/B20_0524_Slice15.tif\", size=(3463, 8000)\n",
    ")\n",
    "image_2 = read_image(\n",
    "    r\"../../data/cyto_downscaled_3344_3904_evaluation/B20_0525_Slice15.tif\", size=(3668, 7382)\n",
    ")\n",
    "image_1, image_2 = ToTensor()(image_1), ToTensor()(image_2)\n",
    "\n",
    "# Read deformation\n",
    "deformation = load_deformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce matches using homography based on ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(0, 480, 10)\n",
    "y = torch.arange(0, 480, 10)\n",
    "\n",
    "X, Y = torch.meshgrid(x, y)\n",
    "matches_crop_1_blueprint = torch.stack((X, Y), dim=2).flatten(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_image_1 = []\n",
    "matches_image_2 = []\n",
    "matches_image_2_not_refined = []\n",
    "padding = 50\n",
    "crop_size=480\n",
    "\n",
    "for y in torch.arange(4860 + padding, 8000 - crop_size - padding, crop_size):\n",
    "    for x in torch.arange(496 + padding, 3463 - crop_size - padding, crop_size):\n",
    "\n",
    "        crop_2_position = deformation[y, x]\n",
    "\n",
    "        crop_coordinate_mapping = create_crop_coordinate_mapping(\n",
    "                    deformation,\n",
    "                    crop_position_image_1=(x, y),\n",
    "                    crop_position_image_2=crop_2_position,\n",
    "                    crop_size=crop_size,\n",
    "                )\n",
    "\n",
    "        matches_crop_1 = matches_crop_1_blueprint\n",
    "        matches_crop_1 = matches_crop_1.long()\n",
    "        \n",
    "        ground_truth_matches_crop_2 = crop_coordinate_mapping[matches_crop_1[:, 1], matches_crop_1[:, 0]]\n",
    "        mask = ground_truth_matches_crop_2!=-1\n",
    "        mask = mask.all(dim=1)\n",
    "\n",
    "        matches_crop_1 = matches_crop_1[mask]\n",
    "        ground_truth_matches_crop_2 = ground_truth_matches_crop_2[mask]\n",
    "\n",
    "        homography = find_homography_dlt(matches_crop_1.unsqueeze(0).float(), ground_truth_matches_crop_2.unsqueeze(0).float())\n",
    "        predicted_matches_crop_2 = (homography@(torch.column_stack([matches_crop_1, torch.ones_like(matches_crop_1[:, 0])]).float().T))[0].T[:, :2]\n",
    "\n",
    "        matches_crop_1 += torch.tensor([x, y])\n",
    "        predicted_matches_crop_2 += crop_2_position\n",
    "\n",
    "        matches_image_1.append(matches_crop_1)\n",
    "        matches_image_2.append(predicted_matches_crop_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_image_1 = torch.cat(matches_image_1)\n",
    "matches_image_2 = torch.cat(matches_image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.miscellaneous.evaluation import evaluate_test_image_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.922843225529942, 2.554981231689453)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    number_of_matches,\n",
    "    average_distance,\n",
    "    match_precision,\n",
    "    auc,\n",
    "    matches_per_patch,\n",
    "    entropy,\n",
    ") = evaluate_test_image_pair(matches_image_1, matches_image_2, deformation)\n",
    "auc, average_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match whole image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.arange(4860 + padding, 8000 - crop_size - padding, 10)\n",
    "x = torch.arange(496 + padding, 3463 - crop_size - padding, 10)\n",
    "\n",
    "X, Y = torch.meshgrid(x, y)\n",
    "matches_image_1 = torch.stack((X, Y), dim=2).flatten(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches_crop_2 = deformation[matches_image_1[:, 1], matches_image_1[:, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "homography = find_homography_dlt(matches_image_1.unsqueeze(0).float(), ground_truth_matches_crop_2.unsqueeze(0).float())\n",
    "predicted_matches_crop_2 = (homography@(torch.column_stack([matches_image_1, torch.ones_like(matches_image_1[:, 0])]).float().T))[0].T[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([61184, 2]), torch.Size([61184, 2]))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_image_1.shape, predicted_matches_crop_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 162.3780059814453)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    number_of_matches,\n",
    "    average_distance,\n",
    "    match_precision,\n",
    "    auc,\n",
    "    matches_per_patch,\n",
    "    entropy,\n",
    ") = evaluate_test_image_pair(matches_image_1, predicted_matches_crop_2, deformation)\n",
    "auc, average_distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
