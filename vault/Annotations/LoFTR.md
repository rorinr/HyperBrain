annotation-target::LoFTR.pdf

>%%
>```annotation-json
>{"created":"2023-11-02T12:39:52.154Z","text":"feature detectors detect mainly salient points, which may lead to too few points\n","updated":"2023-11-02T12:39:52.154Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":1928,"end":2166},{"type":"TextQuoteSelector","exact":"The use of a feature detector reduces the search space ofmatching, and the resulted sparse correspondences are suffi-cient for most tasks, e.g., camera pose estimation. However,a feature detector may fail to extract enough interest points","prefix":"phisticated matching algorithms.","suffix":"∗The first three authors contrib"}]}]}
>```
>%%
>*%%PREFIX%%phisticated matching algorithms.%%HIGHLIGHT%% ==The use of a feature detector reduces the search space ofmatching, and the resulted sparse correspondences are suffi-cient for most tasks, e.g., camera pose estimation. However,a feature detector may fail to extract enough interest points== %%POSTFIX%%∗The first three authors contrib*
>%%LINK%%[[#^ltcopvf3ped|show annotation]]
>%%COMMENT%%
>feature detectors detect mainly salient points, which may lead to too few points
>
>%%TAGS%%
>
^ltcopvf3ped


>%%
>```annotation-json
>{"created":"2023-11-02T12:42:23.287Z","updated":"2023-11-02T12:42:23.287Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":3902,"end":4075},{"type":"TextQuoteSelector","exact":"can be distinguished according to their relative po-sitions to the edges. This observation tells us that a largereceptive field in the feature extraction network is crucial.","prefix":"80v1  [cs.CV]  1 Apr 2021Fig. 1 ","suffix":"Motivated by the above observati"}]}]}
>```
>%%
>*%%PREFIX%%80v1  [cs.CV]  1 Apr 2021Fig. 1%%HIGHLIGHT%% ==can be distinguished according to their relative po-sitions to the edges. This observation tells us that a largereceptive field in the feature extraction network is crucial.== %%POSTFIX%%Motivated by the above observati*
>%%LINK%%[[#^m2dxf04xfxs|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^m2dxf04xfxs


>%%
>```annotation-json
>{"created":"2023-11-02T12:43:50.473Z","updated":"2023-11-02T12:43:50.473Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":4167,"end":4214},{"type":"TextQuoteSelector","exact":"detector-freeapproach to local feature matching","prefix":"re TRansformer (LoFTR), a novel ","suffix":". Inspired by seminalwork SuperG"}]}]}
>```
>%%
>*%%PREFIX%%re TRansformer (LoFTR), a novel%%HIGHLIGHT%% ==detector-freeapproach to local feature matching== %%POSTFIX%%. Inspired by seminalwork SuperG*
>%%LINK%%[[#^l9m95dtkarl|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^l9m95dtkarl


>%%
>```annotation-json
>{"created":"2023-11-02T12:44:15.171Z","updated":"2023-11-02T12:44:15.171Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":4216,"end":4405},{"type":"TextQuoteSelector","exact":"Inspired by seminalwork SuperGlue [37], we use Transformer [48] with selfand cross attention layers to process (transform) the denselocal features extracted from the convolutional backbone.","prefix":"oach to local feature matching. ","suffix":"Dense matches are first extracte"}]}]}
>```
>%%
>*%%PREFIX%%oach to local feature matching.%%HIGHLIGHT%% ==Inspired by seminalwork SuperGlue [37], we use Transformer [48] with selfand cross attention layers to process (transform) the denselocal features extracted from the convolutional backbone.== %%POSTFIX%%Dense matches are first extracte*
>%%LINK%%[[#^jolmvrh9e|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^jolmvrh9e


>%%
>```annotation-json
>{"created":"2023-11-02T12:48:17.926Z","updated":"2023-11-02T12:48:17.926Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":4682,"end":4831},{"type":"TextQuoteSelector","exact":"The globalreceptive field and positional encoding of Transformer en-able the transformed feature representations to be context-and position-dependent","prefix":"h a correlation-based approach. ","suffix":". By interleaving the self and c"}]}]}
>```
>%%
>*%%PREFIX%%h a correlation-based approach.%%HIGHLIGHT%% ==The globalreceptive field and positional encoding of Transformer en-able the transformed feature representations to be context-and position-dependent== %%POSTFIX%%. By interleaving the self and c*
>%%LINK%%[[#^sn7nir9uhyt|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^sn7nir9uhyt


>%%
>```annotation-json
>{"created":"2023-11-02T12:51:49.477Z","updated":"2023-11-02T12:51:49.477Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":7197,"end":7349},{"type":"TextQuoteSelector","exact":"However, being a detector-dependent method,it has the fundamental drawback of being unable to detectrepeatable interest points in indistinctive regions.","prefix":"e art in local featurematching. ","suffix":" The at-tention range in SuperGl"}]}]}
>```
>%%
>*%%PREFIX%%e art in local featurematching.%%HIGHLIGHT%% ==However, being a detector-dependent method,it has the fundamental drawback of being unable to detectrepeatable interest points in indistinctive regions.== %%POSTFIX%%The at-tention range in SuperGl*
>%%LINK%%[[#^gc90ozkizos|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^gc90ozkizos


>%%
>```annotation-json
>{"created":"2023-11-02T12:52:17.933Z","updated":"2023-11-02T12:52:17.933Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":7437,"end":7654},{"type":"TextQuoteSelector","exact":"Our work is inspired by SuperGlue interms of using self and cross attention in GNN for messagepassing between two sets of descriptors, but we propose adetector-free design to avoid the drawbacks of feature de-tectors.","prefix":"e detectedinterest points only. ","suffix":" We also use an efficient varian"}]}]}
>```
>%%
>*%%PREFIX%%e detectedinterest points only.%%HIGHLIGHT%% ==Our work is inspired by SuperGlue interms of using self and cross attention in GNN for messagepassing between two sets of descriptors, but we propose adetector-free design to avoid the drawbacks of feature de-tectors.== %%POSTFIX%%We also use an efficient varian*
>%%LINK%%[[#^ms5px54o3gq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ms5px54o3gq


>%%
>```annotation-json
>{"created":"2023-11-02T12:55:03.454Z","updated":"2023-11-02T12:55:03.454Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":7798,"end":7918},{"type":"TextQuoteSelector","exact":"Detector-freemethods remove the feature detector phase and directly pro-duce dense descriptors or dense feature matches.","prefix":"or-free Local Feature Matching. ","suffix":" The ideaof dense features match"}]}]}
>```
>%%
>*%%PREFIX%%or-free Local Feature Matching.%%HIGHLIGHT%% ==Detector-freemethods remove the feature detector phase and directly pro-duce dense descriptors or dense feature matches.== %%POSTFIX%%The ideaof dense features match*
>%%LINK%%[[#^sww3blkmo8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^sww3blkmo8


>%%
>```annotation-json
>{"created":"2023-11-02T15:57:01.463Z","updated":"2023-11-02T15:57:01.463Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":35081,"end":35294},{"type":"TextQuoteSelector","exact":"multi-level features from both images. We use  ̃FA and  ̃FBto denote the coarse-level features at 1/8 of the original im-age dimension, and ˆFA and ˆFB the fine-level features at 1/2of the original image dimension","prefix":"he local feature CNN) to extract","suffix":".Convolutional Neural Networks ("}]}]}
>```
>%%
>*%%PREFIX%%he local feature CNN) to extract%%HIGHLIGHT%% ==multi-level features from both images. We use  ̃FA and  ̃FBto denote the coarse-level features at 1/8 of the original im-age dimension, and ˆFA and ˆFB the fine-level features at 1/2of the original image dimension== %%POSTFIX%%.Convolutional Neural Networks (*
>%%LINK%%[[#^nqmx99z0lur|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^nqmx99z0lur


>%%
>```annotation-json
>{"created":"2023-11-03T09:12:19.592Z","updated":"2023-11-03T09:12:19.592Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":322,"end":442},{"type":"TextQuoteSelector","exact":"we propose to firstestablish pixel-wise dense matches at a coarse level andlater refine the good matches at a fine level","prefix":"ion, and matching sequentially, ","suffix":". In contrastto dense methods th"}]}]}
>```
>%%
>*%%PREFIX%%ion, and matching sequentially,%%HIGHLIGHT%% ==we propose to firstestablish pixel-wise dense matches at a coarse level andlater refine the good matches at a fine level== %%POSTFIX%%. In contrastto dense methods th*
>%%LINK%%[[#^jucgkuh0nrr|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^jucgkuh0nrr


>%%
>```annotation-json
>{"created":"2023-11-03T09:13:48.554Z","updated":"2023-11-03T09:13:48.554Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":648,"end":759},{"type":"TextQuoteSelector","exact":"global receptive field provided by Trans-former enables our method to produce dense matches inlow-texture areas","prefix":" conditioned onboth images. The ","suffix":", where feature detectors usuall"}]}]}
>```
>%%
>*%%PREFIX%%conditioned onboth images. The%%HIGHLIGHT%% ==global receptive field provided by Trans-former enables our method to produce dense matches inlow-texture areas== %%POSTFIX%%, where feature detectors usuall*
>%%LINK%%[[#^vm0viwt82dm|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^vm0viwt82dm


>%%
>```annotation-json
>{"created":"2023-11-03T10:22:30.061Z","updated":"2023-11-03T10:22:30.061Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":35295,"end":35601},{"type":"TextQuoteSelector","exact":"Convolutional Neural Networks (CNNs) possess the in-ductive bias of translation equivariance and locality, whichare well suited for local feature extraction. The downsam-pling introduced by the CNN also reduces the input lengthof the LoFTR module, which is crucial to ensure a manage-able computation cost.","prefix":"of the original image dimension.","suffix":"3.2. Local Feature Transformer ("}]}]}
>```
>%%
>*%%PREFIX%%of the original image dimension.%%HIGHLIGHT%% ==Convolutional Neural Networks (CNNs) possess the in-ductive bias of translation equivariance and locality, whichare well suited for local feature extraction. The downsam-pling introduced by the CNN also reduces the input lengthof the LoFTR module, which is crucial to ensure a manage-able computation cost.== %%POSTFIX%%3.2. Local Feature Transformer (*
>%%LINK%%[[#^nig4gi80kan|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^nig4gi80kan


>%%
>```annotation-json
>{"created":"2023-11-03T10:22:38.203Z","updated":"2023-11-03T10:22:38.203Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":34987,"end":35081},{"type":"TextQuoteSelector","exact":"standard convolutional architecture withFPN [22] (denoted as the local feature CNN) to extract","prefix":"ocal Feature ExtractionWe use a ","suffix":"multi-level features from both i"}]}]}
>```
>%%
>*%%PREFIX%%ocal Feature ExtractionWe use a%%HIGHLIGHT%% ==standard convolutional architecture withFPN [22] (denoted as the local feature CNN) to extract== %%POSTFIX%%multi-level features from both i*
>%%LINK%%[[#^g56qo840ada|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^g56qo840ada


>%%
>```annotation-json
>{"created":"2023-11-03T15:19:48.988Z","updated":"2023-11-03T15:19:48.988Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":35788,"end":35892},{"type":"TextQuoteSelector","exact":"Intuitively, the LoFTR moduletransforms the features into feature representations that areeasy to match.","prefix":"ontextdependent local features. ","suffix":" We denote the transformed featu"}]}]}
>```
>%%
>*%%PREFIX%%ontextdependent local features.%%HIGHLIGHT%% ==Intuitively, the LoFTR moduletransforms the features into feature representations that areeasy to match.== %%POSTFIX%%We denote the transformed featu*
>%%LINK%%[[#^2p6my2ls4j2|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^2p6my2ls4j2


>%%
>```annotation-json
>{"created":"2023-11-03T15:26:22.833Z","updated":"2023-11-03T15:26:22.833Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":57445,"end":57703},{"type":"TextQuoteSelector","exact":"For self-attention layers, the input features fi and fj (shown inFig. 3) are the same (either  ̃FA or  ̃FB). For cross-attentionlayers, the input features fi and fj are either (  ̃FA and ̃FB) or (  ̃FB and  ̃FA) depending on the direction of cross-attention.","prefix":"ion and Cross-attention Layers. ","suffix":" Following [37], we interleave t"}]}]}
>```
>%%
>*%%PREFIX%%ion and Cross-attention Layers.%%HIGHLIGHT%% ==For self-attention layers, the input features fi and fj (shown inFig. 3) are the same (either  ̃FA or  ̃FB). For cross-attentionlayers, the input features fi and fj are either (  ̃FA and ̃FB) or (  ̃FB and  ̃FA) depending on the direction of cross-attention.== %%POSTFIX%%Following [37], we interleave t*
>%%LINK%%[[#^seoq85ix89q|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^seoq85ix89q


>%%
>```annotation-json
>{"created":"2023-11-03T15:30:36.880Z","updated":"2023-11-03T15:30:36.880Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":58338,"end":58626},{"type":"TextQuoteSelector","exact":"We can also apply softmaxon both dimensions (referred to as dual-softmax in the fol-lowing) of S to obtain the probability of soft mutual nearestneighbor matching. Formally, when using dual-softmax, thematching probability Pc is obtained by:Pc(i,j) = softmax (S(i,·))j ·softmax (S(·,j))i ","prefix":"lassignment problem as in [37]. ","suffix":".Match Selection. Based on the c"}]}]}
>```
>%%
>*%%PREFIX%%lassignment problem as in [37].%%HIGHLIGHT%% ==We can also apply softmaxon both dimensions (referred to as dual-softmax in the fol-lowing) of S to obtain the probability of soft mutual nearestneighbor matching. Formally, when using dual-softmax, thematching probability Pc is obtained by:Pc(i,j) = softmax (S(i,·))j ·softmax (S(·,j))i== %%POSTFIX%%.Match Selection. Based on the c*
>%%LINK%%[[#^1shmp4fu8c5|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^1shmp4fu8c5


>%%
>```annotation-json
>{"created":"2023-11-03T15:33:20.464Z","updated":"2023-11-03T15:33:20.464Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":58125,"end":58235},{"type":"TextQuoteSelector","exact":"Thescore matrix S between the transformed features is first cal-culated by S(i,j) = 1τ ·〈 ̃FAtr(i),  ̃FBtr(j)〉","prefix":"dual-softmax operator [34, 47]. ","suffix":". When matchingwith OT, −S can b"}]}]}
>```
>%%
>*%%PREFIX%%dual-softmax operator [34, 47].%%HIGHLIGHT%% ==Thescore matrix S between the transformed features is first cal-culated by S(i,j) = 1τ ·〈 ̃FAtr(i),  ̃FBtr(j)〉== %%POSTFIX%%. When matchingwith OT, −S can b*
>%%LINK%%[[#^64wao818pc|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^64wao818pc


>%%
>```annotation-json
>{"created":"2023-11-03T15:38:39.401Z","updated":"2023-11-03T15:38:39.401Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":58644,"end":58899},{"type":"TextQuoteSelector","exact":"Based on the confidence matrix Pc, weselect matches with confidence higher than a threshold ofθc, and further enforce the mutual nearest neighbor (MNN)criteria, which filters possible outlier coarse matches. Wedenote the coarse-level match predictions as:","prefix":"max (S(·,j))i .Match Selection. ","suffix":"Mc = {( ̃i, ̃j) |∀( ̃i, ̃j) ∈MNN"}]}]}
>```
>%%
>*%%PREFIX%%max (S(·,j))i .Match Selection.%%HIGHLIGHT%% ==Based on the confidence matrix Pc, weselect matches with confidence higher than a threshold ofθc, and further enforce the mutual nearest neighbor (MNN)criteria, which filters possible outlier coarse matches. Wedenote the coarse-level match predictions as:== %%POSTFIX%%Mc = {( ̃i, ̃j) |∀( ̃i, ̃j) ∈MNN*
>%%LINK%%[[#^dxl4mxjfr2e|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^dxl4mxjfr2e


>%%
>```annotation-json
>{"created":"2023-11-03T16:13:10.860Z","updated":"2023-11-03T16:13:10.860Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":59934,"end":59945},{"type":"TextQuoteSelector","exact":"Supervision","prefix":"inal fine-level matches Mf.3.5. ","suffix":"The final loss consists of the l"}]}]}
>```
>%%
>*%%PREFIX%%inal fine-level matches Mf.3.5.%%HIGHLIGHT%% ==Supervision== %%POSTFIX%%The final loss consists of the l*
>%%LINK%%[[#^pb0wjp31j0i|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^pb0wjp31j0i


>%%
>```annotation-json
>{"created":"2023-11-03T16:14:26.980Z","updated":"2023-11-03T16:14:26.980Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":62100,"end":62230},{"type":"TextQuoteSelector","exact":"FAtr and  ̃FBtr are upsampled and concate-nated with ˆFA and ˆFB before passing through the fine-levelLoFTR in the implementation.","prefix":". Window sizew is equal to 5.  ̃","suffix":" The full model with dual-softma"}]}]}
>```
>%%
>*%%PREFIX%%. Window sizew is equal to 5.  ̃%%HIGHLIGHT%% ==FAtr and  ̃FBtr are upsampled and concate-nated with ˆFA and ˆFB before passing through the fine-levelLoFTR in the implementation.== %%POSTFIX%%The full model with dual-softma*
>%%LINK%%[[#^irwev6lrrmo|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^irwev6lrrmo


>%%
>```annotation-json
>{"created":"2023-11-03T17:17:21.299Z","updated":"2023-11-03T17:17:21.299Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":63298,"end":63318},{"type":"TextQuoteSelector","exact":"Evaluation protocol.","prefix":"able matching with dual-softmax.","suffix":" In every test sequence, one ref"}]}]}
>```
>%%
>*%%PREFIX%%able matching with dual-softmax.%%HIGHLIGHT%% ==Evaluation protocol.== %%POSTFIX%%In every test sequence, one ref*
>%%LINK%%[[#^aw5apii4ivq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^aw5apii4ivq


>%%
>```annotation-json
>{"created":"2023-11-03T17:18:48.489Z","updated":"2023-11-03T17:18:48.489Z","document":{"title":"LoFTR.pdf","link":[{"href":"urn:x-pdf:4fd1751e2558a84d63f76aaf965db625"},{"href":"vault:/Sources/Image Matching/LoFTR.pdf"}],"documentFingerprint":"4fd1751e2558a84d63f76aaf965db625"},"uri":"vault:/Sources/Image Matching/LoFTR.pdf","target":[{"source":"vault:/Sources/Image Matching/LoFTR.pdf","selector":[{"type":"TextPositionSelector","start":34429,"end":34470},{"type":"TextQuoteSelector","exact":"Many efficient vari-ants [42, 18, 17, 5] ","prefix":" between query and key vectors. ","suffix":"are proposed recently in the con"}]}]}
>```
>%%
>*%%PREFIX%%between query and key vectors.%%HIGHLIGHT%% ==Many efficient vari-ants [42, 18, 17, 5]== %%POSTFIX%%are proposed recently in the con*
>%%LINK%%[[#^q8m8lqd27i|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^q8m8lqd27i
