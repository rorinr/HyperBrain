annotation-target::CasMTR.pdf

>%%
>```annotation-json
>{"created":"2023-10-20T04:43:00.770Z","text":"Attention learning more expensive in image data for smaller patches ","updated":"2023-10-20T04:43:00.770Z","document":{"title":"CasMTR.pdf","link":[{"href":"urn:x-pdf:36fc653b3c7124bb0d0b86654a416fc1"},{"href":"vault:/Sources/Image Matching/CasMTR.pdf"}],"documentFingerprint":"36fc653b3c7124bb0d0b86654a416fc1"},"uri":"vault:/Sources/Image Matching/CasMTR.pdf","target":[{"source":"vault:/Sources/Image Matching/CasMTR.pdf","selector":[{"type":"TextPositionSelector","start":515,"end":678},{"type":"TextQuoteSelector","exact":"But correlations produced bytransformer-based methods are spatially limited to the cen-ter of source views’ coarse patches, because of the costlyattention learning","prefix":"NeuralNetwork (CNN) based ones. ","suffix":". In this work, we rethink this "}]}]}
>```
>%%
>*%%PREFIX%%NeuralNetwork (CNN) based ones.%%HIGHLIGHT%% ==But correlations produced bytransformer-based methods are spatially limited to the cen-ter of source views’ coarse patches, because of the costlyattention learning== %%POSTFIX%%. In this work, we rethink this*
>%%LINK%%[[#^lfch5qw3gt|show annotation]]
>%%COMMENT%%
>Attention learning more expensive in image data for smaller patches 
>%%TAGS%%
>
^lfch5qw3gt


>%%
>```annotation-json
>{"created":"2023-10-20T04:46:13.020Z","updated":"2023-10-20T04:46:13.020Z","document":{"title":"CasMTR.pdf","link":[{"href":"urn:x-pdf:36fc653b3c7124bb0d0b86654a416fc1"},{"href":"vault:/Sources/Image Matching/CasMTR.pdf"}],"documentFingerprint":"36fc653b3c7124bb0d0b86654a416fc1"},"uri":"vault:/Sources/Image Matching/CasMTR.pdf","target":[{"source":"vault:/Sources/Image Matching/CasMTR.pdf","selector":[{"type":"TextPositionSelector","start":3396,"end":3498},{"type":"TextQuoteSelector","exact":"But such detector-based CNNs suffer fromlimited receptive fields and search space, as noticed in [43].","prefix":"t improvements inthis pipeline. ","suffix":"To solve this issue, transformer"}]}]}
>```
>%%
>*%%PREFIX%%t improvements inthis pipeline.%%HIGHLIGHT%% ==But such detector-based CNNs suffer fromlimited receptive fields and search space, as noticed in [43].== %%POSTFIX%%To solve this issue, transformer*
>%%LINK%%[[#^ovi9b3gvnwa|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ovi9b3gvnwa


>%%
>```annotation-json
>{"created":"2023-10-20T04:53:34.790Z","updated":"2023-10-20T04:53:34.790Z","document":{"title":"CasMTR.pdf","link":[{"href":"urn:x-pdf:36fc653b3c7124bb0d0b86654a416fc1"},{"href":"vault:/Sources/Image Matching/CasMTR.pdf"}],"documentFingerprint":"36fc653b3c7124bb0d0b86654a416fc1"},"uri":"vault:/Sources/Image Matching/CasMTR.pdf","target":[{"source":"vault:/Sources/Image Matching/CasMTR.pdf","selector":[{"type":"TextPositionSelector","start":3519,"end":3600},{"type":"TextQuoteSelector","exact":"transformer-based detector-free meth-ods have emerged as more robust alternatives","prefix":"ed in [43].To solve this issue, ","suffix":", demon-strating impressive matc"}]}]}
>```
>%%
>*%%PREFIX%%ed in [43].To solve this issue,%%HIGHLIGHT%% ==transformer-based detector-free meth-ods have emerged as more robust alternatives== %%POSTFIX%%, demon-strating impressive matc*
>%%LINK%%[[#^osxuzzf5gj|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^osxuzzf5gj


>%%
>```annotation-json
>{"created":"2023-11-01T10:47:34.173Z","updated":"2023-11-01T10:47:34.173Z","document":{"title":"CasMTR.pdf","link":[{"href":"urn:x-pdf:36fc653b3c7124bb0d0b86654a416fc1"},{"href":"vault:/Sources/Image Matching/CasMTR.pdf"}],"documentFingerprint":"36fc653b3c7124bb0d0b86654a416fc1"},"uri":"vault:/Sources/Image Matching/CasMTR.pdf","target":[{"source":"vault:/Sources/Image Matching/CasMTR.pdf","selector":[{"type":"TextPositionSelector","start":3706,"end":3883},{"type":"TextQuoteSelector","exact":"high computa-tional cost of attention limits transformer-based methodsto ‘semi-dense’ matching, where source matching points arespaced apart at intervals of coarse feature space","prefix":"3, 18, 47, 57, 4]. However, the ","suffix":", as shownin Fig.1(a,d). Such se"}]}]}
>```
>%%
>*%%PREFIX%%3, 18, 47, 57, 4]. However, the%%HIGHLIGHT%% ==high computa-tional cost of attention limits transformer-based methodsto ‘semi-dense’ matching, where source matching points arespaced apart at intervals of coarse feature space== %%POSTFIX%%, as shownin Fig.1(a,d). Such se*
>%%LINK%%[[#^o3rv5epgxu|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^o3rv5epgxu


>%%
>```annotation-json
>{"created":"2023-11-09T11:30:23.357Z","updated":"2023-11-09T11:30:23.357Z","document":{"title":"CasMTR.pdf","link":[{"href":"urn:x-pdf:36fc653b3c7124bb0d0b86654a416fc1"},{"href":"vault:/Sources/Image Matching/CasMTR.pdf"}],"documentFingerprint":"36fc653b3c7124bb0d0b86654a416fc1"},"uri":"vault:/Sources/Image Matching/CasMTR.pdf","target":[{"source":"vault:/Sources/Image Matching/CasMTR.pdf","selector":[{"type":"TextPositionSelector","start":11283,"end":11447},{"type":"TextQuoteSelector","exact":"Moreover, it is non-trivial to extend these transformer-basedmatching solutions into dense and high-resolution cases be-cause of the heavy computation of attention.","prefix":"ation for low-resolution images.","suffix":"Coarse-to-fine Learning. The eff"}]}]}
>```
>%%
>*%%PREFIX%%ation for low-resolution images.%%HIGHLIGHT%% ==Moreover, it is non-trivial to extend these transformer-basedmatching solutions into dense and high-resolution cases be-cause of the heavy computation of attention.== %%POSTFIX%%Coarse-to-fine Learning. The eff*
>%%LINK%%[[#^zzcw89eyzs|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^zzcw89eyzs


>%%
>```annotation-json
>{"created":"2023-11-09T11:32:50.228Z","updated":"2023-11-09T11:32:50.228Z","document":{"title":"CasMTR.pdf","link":[{"href":"urn:x-pdf:36fc653b3c7124bb0d0b86654a416fc1"},{"href":"vault:/Sources/Image Matching/CasMTR.pdf"}],"documentFingerprint":"36fc653b3c7124bb0d0b86654a416fc1"},"uri":"vault:/Sources/Image Matching/CasMTR.pdf","target":[{"source":"vault:/Sources/Image Matching/CasMTR.pdf","selector":[{"type":"TextPositionSelector","start":12885,"end":13314},{"type":"TextQuoteSelector","exact":"LoFTR uses a local feature CNN to extractcoarse (1/8) and fine (1/2) feature maps from image pairs.Then interlaced self/cross-attention modules are leveragedto learn coarse-level matching predictions. Additionally,LoFTR utilizes a refinement module to model sub-pixelmatch prediction in fine-level features. However, the sourcepoint of each matched pair is still restricted at the coarselevel (1/8), which limits the performance.","prefix":"ne in the example ofLoFTR [43]. ","suffix":" Some follow-ups [47, 4] improve"}]}]}
>```
>%%
>*%%PREFIX%%ne in the example ofLoFTR [43].%%HIGHLIGHT%% ==LoFTR uses a local feature CNN to extractcoarse (1/8) and fine (1/2) feature maps from image pairs.Then interlaced self/cross-attention modules are leveragedto learn coarse-level matching predictions. Additionally,LoFTR utilizes a refinement module to model sub-pixelmatch prediction in fine-level features. However, the sourcepoint of each matched pair is still restricted at the coarselevel (1/8), which limits the performance.== %%POSTFIX%%Some follow-ups [47, 4] improve*
>%%LINK%%[[#^f4ffnv2kk5|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^f4ffnv2kk5
