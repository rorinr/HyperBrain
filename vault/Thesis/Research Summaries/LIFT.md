- Introduce a novel architecture for the whole processing chain: detecting features, computing features orientation and extracting robust representations ([[Annotations/LIFT#^19htf4xtbmy]])
- Ground-truth comes from SfM: Given two different images of the same scene, the SfM algorithm finds corresponding points in both, representing the same point in 3D ([[Annotations/LIFT#^4rmfdfvp8ba]])
- Detector runs separately from orientation estimator and descriptor at test time; Detector runs over the whole image in scale space ([[Annotations/LIFT#^ghons5avkcp]])
- Select small patches from the keypoints used by a sfm pipeline; assume a patch contain only one dominant local feature -> Find the most distinctive point in each patch ([[Annotations/LIFT#^mwg0axgvtu]])
- Train using 4 patches: 2 of them correspond to different view of the same physical point (found by sfm), one patch correspond to a different physical point (found by sfm) and a fourth patch which does not contain any feature point ([[Annotations/LIFT#^mwg0axgvtu]])
- Given a patch P, the detector provides score map S with argmax location x and a smaller patch p is extracted and further processed. After that the not-learned spatial transformer rotate the patch accordingly and pass it to the descriptor. Location x and orientation are learned implicitly ([[Annotations/LIFT#^kfb60lmsupq]])
- Learn first the descriptor, then orientation estimator and then the detector ([[Annotations/LIFT#^0cr76r5jl0b7]])
- Descriptor loss: L2, shall be zero for corresponding patches and ~C or higher for non-corresponding ones ([[Annotations/LIFT#^uught21bjy9]])
- They used hard-mining, which means that they only backpropagate the datapoints with high loss. the mining ratio r defines how many of the forward-propagated datapoints to use. r=1 means to use all of them, r=2 half of them (the half with the higher losses) and so on ([[Annotations/LIFT#^h08bjite7ji]])
- Use the already trained descriptor for train the orientation estimator using only positive pairs, ie image patches showing the same physical 3d point. The orientation estimator is trained by minimizing the euclidean distance of the embedding-vectors (embedding vectors = output of descriptor) ([[Annotations/LIFT#^2kqxs5u0oqf]])
- Train detector using whole pipeline ([[Annotations/LIFT#^g3fi6sx6ic]])
- Train the detector by classifying if keypoint in patch and by minimizing the distance between corresponding image patches ([[Annotations/LIFT#^lt0qfxxhpnm]])
- At test-time, decoupling the detector from the rest of the network saves compute ([[Annotations/LIFT#^32uj5odnuu]])
### LIFT questions
- Do they really train on pre-selected keypoints?
- Spatial transformer
- non-local maximum suppression
- how important is orientation estimation in the first place, what do they mean by orientation?
- Regarding the orientation estimator: What did they train and what not?